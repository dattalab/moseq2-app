{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Moseq2-App\" data-toc-modified-id=\"Moseq2-App-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Moseq2 App</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Resources\" data-toc-modified-id=\"Resources-1.0.1\"><span class=\"toc-item-num\">1.0.1&nbsp;&nbsp;</span>Resources</a></span></li><li><span><a href=\"#Feedback\" data-toc-modified-id=\"Feedback-1.0.2\"><span class=\"toc-item-num\">1.0.2&nbsp;&nbsp;</span>Feedback</a></span></li><li><span><a href=\"#Data-Acquisition-Overview\" data-toc-modified-id=\"Data-Acquisition-Overview-1.0.3\"><span class=\"toc-item-num\">1.0.3&nbsp;&nbsp;</span>Data Acquisition Overview</a></span></li></ul></li><li><span><a href=\"#Notebook-Setup\" data-toc-modified-id=\"Notebook-Setup-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Notebook Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Check-to-see-if-you're-running-python-from-the-correct-conda-enviornment\" data-toc-modified-id=\"Check-to-see-if-you're-running-python-from-the-correct-conda-enviornment-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Check to see if you're running python from the correct conda enviornment</a></span></li><li><span><a href=\"#Check-if-the-dependencies-are-found\" data-toc-modified-id=\"Check-if-the-dependencies-are-found-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Check if the dependencies are found</a></span></li><li><span><a href=\"#Alternative-Notebook-Setup\" data-toc-modified-id=\"Alternative-Notebook-Setup-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Alternative Notebook Setup</a></span></li></ul></li><li><span><a href=\"#Data-file-organization\" data-toc-modified-id=\"Data-file-organization-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Data file organization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Notebook-Progress-File\" data-toc-modified-id=\"Notebook-Progress-File-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Notebook Progress File</a></span></li><li><span><a href=\"#Restore-Progress-Variables\" data-toc-modified-id=\"Restore-Progress-Variables-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Restore Progress Variables</a></span></li><li><span><a href=\"#Generate-Configuration-Files\" data-toc-modified-id=\"Generate-Configuration-Files-1.2.3\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>Generate Configuration Files</a></span></li><li><span><a href=\"#Download-a-Flip-File\" data-toc-modified-id=\"Download-a-Flip-File-1.2.4\"><span class=\"toc-item-num\">1.2.4&nbsp;&nbsp;</span>Download a Flip File</a></span></li></ul></li></ul></li><li><span><a href=\"#Raw-Data-Extraction\" data-toc-modified-id=\"Raw-Data-Extraction-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Raw Data Extraction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Interactive-ROI-Detection-Tool\" data-toc-modified-id=\"Interactive-ROI-Detection-Tool-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Interactive ROI Detection Tool</a></span><ul class=\"toc-item\"><li><span><a href=\"#Basic-Usage\" data-toc-modified-id=\"Basic-Usage-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Basic Usage</a></span></li><li><span><a href=\"#Troubleshooting\" data-toc-modified-id=\"Troubleshooting-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Troubleshooting</a></span><ul class=\"toc-item\"><li><span><a href=\"#ROI-Not-Detected-Correctly\" data-toc-modified-id=\"ROI-Not-Detected-Correctly-2.1.2.1\"><span class=\"toc-item-num\">2.1.2.1&nbsp;&nbsp;</span>ROI Not Detected Correctly</a></span></li><li><span><a href=\"#Too-Much-Noise-Around-Mouse-Body\" data-toc-modified-id=\"Too-Much-Noise-Around-Mouse-Body-2.1.2.2\"><span class=\"toc-item-num\">2.1.2.2&nbsp;&nbsp;</span>Too Much Noise Around Mouse Body</a></span></li><li><span><a href=\"#Flagged-Session\" data-toc-modified-id=\"Flagged-Session-2.1.2.3\"><span class=\"toc-item-num\">2.1.2.3&nbsp;&nbsp;</span>Flagged Session</a></span></li></ul></li><li><span><a href=\"#Editable-Configuration-Parameter-Details\" data-toc-modified-id=\"Editable-Configuration-Parameter-Details-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Editable Configuration Parameter Details</a></span></li><li><span><a href=\"#Restore-Progress-Variables\" data-toc-modified-id=\"Restore-Progress-Variables-2.1.4\"><span class=\"toc-item-num\">2.1.4&nbsp;&nbsp;</span>Restore Progress Variables</a></span></li></ul></li><li><span><a href=\"#Extract-Session(s)\" data-toc-modified-id=\"Extract-Session(s)-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Extract Session(s)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Run-Extraction-Validation-Tests-(optional)\" data-toc-modified-id=\"Run-Extraction-Validation-Tests-(optional)-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Run Extraction Validation Tests (optional)</a></span></li><li><span><a href=\"#Review-Extraction-Output-(optional)\" data-toc-modified-id=\"Review-Extraction-Output-(optional)-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Review Extraction Output (optional)</a></span></li></ul></li><li><span><a href=\"#Aggregate-your-results-into-one-folder-and-generate-an-index-file.\" data-toc-modified-id=\"Aggregate-your-results-into-one-folder-and-generate-an-index-file.-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Aggregate your results into one folder and generate an index file.</a></span></li><li><span><a href=\"#Specify-Groups\" data-toc-modified-id=\"Specify-Groups-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Specify Groups</a></span></li><li><span><a href=\"#Further-Extraction-Diagnostics\" data-toc-modified-id=\"Further-Extraction-Diagnostics-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Further Extraction Diagnostics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Compute-Scalar-Summary-(Diagnostics)\" data-toc-modified-id=\"Compute-Scalar-Summary-(Diagnostics)-2.5.1\"><span class=\"toc-item-num\">2.5.1&nbsp;&nbsp;</span>Compute Scalar Summary (Diagnostics)</a></span></li><li><span><a href=\"#Plot-Position-Heatmaps-For-Each-Session-(Diagnostics)\" data-toc-modified-id=\"Plot-Position-Heatmaps-For-Each-Session-(Diagnostics)-2.5.2\"><span class=\"toc-item-num\">2.5.2&nbsp;&nbsp;</span>Plot Position Heatmaps For Each Session (Diagnostics)</a></span></li><li><span><a href=\"#Plot-Group-Mean-Position-Summary-(Optional)\" data-toc-modified-id=\"Plot-Group-Mean-Position-Summary-(Optional)-2.5.3\"><span class=\"toc-item-num\">2.5.3&nbsp;&nbsp;</span>Plot Group Mean Position Summary (Optional)</a></span></li></ul></li></ul></li><li><span><a href=\"#Principal-Component-Analysis-(PCA)\" data-toc-modified-id=\"Principal-Component-Analysis-(PCA)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Principal Component Analysis (PCA)</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Restore-Progress-Variables\" data-toc-modified-id=\"Restore-Progress-Variables-3.0.1\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>Restore Progress Variables</a></span></li></ul></li><li><span><a href=\"#Fitting-PCA\" data-toc-modified-id=\"Fitting-PCA-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Fitting PCA</a></span></li><li><span><a href=\"#Computing-Principal-Component-Scores\" data-toc-modified-id=\"Computing-Principal-Component-Scores-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Computing Principal Component Scores</a></span><ul class=\"toc-item\"><li><span><a href=\"#Computing-Model-Free-Changepoints-(Optional)\" data-toc-modified-id=\"Computing-Model-Free-Changepoints-(Optional)-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Computing Model-Free Changepoints (Optional)</a></span></li></ul></li></ul></li><li><span><a href=\"#ARHMM-Modeling\" data-toc-modified-id=\"ARHMM-Modeling-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>ARHMM Modeling</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Restore-Progress-Variables\" data-toc-modified-id=\"Restore-Progress-Variables-4.0.1\"><span class=\"toc-item-num\">4.0.1&nbsp;&nbsp;</span>Restore Progress Variables</a></span></li></ul></li><li><span><a href=\"#Fitting-the-ARHMM\" data-toc-modified-id=\"Fitting-the-ARHMM-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Fitting the ARHMM</a></span><ul class=\"toc-item\"><li><span><a href=\"#Set-Model-Path\" data-toc-modified-id=\"Set-Model-Path-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Set Model Path</a></span></li><li><span><a href=\"#General-Parameters\" data-toc-modified-id=\"General-Parameters-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>General Parameters</a></span></li><li><span><a href=\"#Train-Model\" data-toc-modified-id=\"Train-Model-4.1.3\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span>Train Model</a></span></li><li><span><a href=\"#Restore-Notebook-Variables\" data-toc-modified-id=\"Restore-Notebook-Variables-4.1.4\"><span class=\"toc-item-num\">4.1.4&nbsp;&nbsp;</span>Restore Notebook Variables</a></span></li></ul></li><li><span><a href=\"#Get-Best-Model-Fit\" data-toc-modified-id=\"Get-Best-Model-Fit-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Get Best Model Fit</a></span></li></ul></li><li><span><a href=\"#Notebook-End\" data-toc-modified-id=\"Notebook-End-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Notebook End</a></span><ul class=\"toc-item\"><li><span><a href=\"#Go-to-the-Interactive-Model-Results.ipynb-Jupyter-Notebook-to-analyze-model-results.\" data-toc-modified-id=\"Go-to-the-Interactive-Model-Results.ipynb-Jupyter-Notebook-to-analyze-model-results.-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Go to the <a href=\"./Interactive-Model-Results-Exploration.ipynb\" target=\"_blank\">Interactive-Model-Results.ipynb</a> Jupyter Notebook to analyze model results.</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moseq2 App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1PxTnCMsrk3hRHPnEjqGDzq1oPkTYfzj0\">\n",
    "\n",
    "MoSeq2 is a software toolkit for unsupervised modeling and characterization of animal behavior. Moseq takes depth recordings of animals as input and outputs a rich description of behavior as a series of reused and stereotyped motifs called 'syllables'. __The model output can be analyzed using the [Interactive Results Exploration](./Interactive-Model-Results-Exploration.ipynb) notebook.__\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Wiki](https://github.com/dattalab/moseq2-app/wiki) with instructions on data acquisition and command line options\n",
    "- PDF documentation of all MoSeq functions: [Extract](https://github.com/dattalab/moseq2-extract/blob/release/Documentation.pdf), [PCA](https://github.com/dattalab/moseq2-pca/blob/release/Documentation.pdf), [Model](https://github.com/dattalab/moseq2-model/blob/release/Documentation.pdf)\n",
    "- Publications:\n",
    "    - [Mapping Sub-Second Structure in Mouse Behavior](http://datta.hms.harvard.edu/wp-content/uploads/2018/01/pub_23.pdf)\n",
    "    - [The Striatum Organizes 3D Behavior via Moment-to-Moment Action Selection](http://datta.hms.harvard.edu/wp-content/uploads/2019/06/Markowitz.final_.pdf)\n",
    "    - [Revealing the structure of pharmacobehavioral space through motion sequencing](https://www.nature.com/articles/s41593-020-00706-3)\n",
    "    - [Q&A: Understanding the composition of behavior](http://datta.hms.harvard.edu/wp-content/uploads/2019/06/Datta-QA.pdf)\n",
    "    \n",
    "### Feedback\n",
    "\n",
    "For general feedback and feature requests, please fill out [this survey](https://forms.gle/FbtEN8E382y8jF3p6).\n",
    "\n",
    "**We recommend recording more than 10 hours of depth video (~1 million frames at 30 frames per second) to ensure quality MoSeq models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1h2GYECyEuTMlM7Rx3Q3lMVBdWEm1F0S5\">\n",
    "\n",
    "### Check to see if you're running python from the correct conda enviornment\n",
    "Run the following cell to check if `moseq2-app` is installed in your current conda kernel. The latest working version number is `0.3.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!moseq2-extract --version\n",
    "!moseq2-pca --version\n",
    "!moseq2-model --version\n",
    "!moseq2-viz --version\n",
    "\n",
    "import sys\n",
    "import moseq2_app\n",
    "\n",
    "print('Python path:', sys.executable)\n",
    "print('MoSeq2 app version:', moseq2_app.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data file organization\n",
    "\n",
    "The currently accepted depth data extensions are:\n",
    "- `.dat` (raw depth files from our kinect2 data acquisition software)\n",
    "- `.tar.gz` (compressed depth files from our kinect2 data acquisition software)\n",
    "- `.avi` (compressed depth files from the `moseq2-extract` command line interface)\n",
    "- `.mkv` (generated from Microsoft's recording software for the Azure Kinect)\n",
    "\n",
    "To run this notebook, create a master folder with **separate subfolders** for each recording session (see example directory structure below). To better organize the extraction, modeling and analysis results, you can copy the MoSeq notbooks to the `Data_Directory` and set the base directory as `./`.\n",
    "\n",
    "```\n",
    ".\n",
    "└── Data_Directory/\n",
    "    ├── MoSeq Jupyter Notebooks (Optional)\n",
    "    ├── session_1/ ** - the folder containing all of a single session's data\n",
    "    ├   ├── depth.dat        # depth data - the recording itself\n",
    "    ├   ├── depth_ts.txt     # timestamps - csv/txt file of the frame timestamps\n",
    "    ├   └── metadata.json    # metadata - json file that contains the rodent's info (group, subjectName, etc.)\n",
    "    ...\n",
    "    ├── session_n/ **\n",
    "    ├   ├── depth.dat\n",
    "    ├   ├── depth_ts.txt\n",
    "    └── └── metadata.json\n",
    "\n",
    "```\n",
    "\n",
    "__Note: if your data was acquired using an Azure Kinect, you will not have `depth_ts.txt` or `metadata.json` in your session directories. MoSeq will automatically generate the necessary files.__ The directory structure would be the following:\n",
    "```\n",
    ".\n",
    "└── Data_Directory/\n",
    "    ├── MoSeq Jupyter Notebooks (Optional)\n",
    "    ├── session_1/ ** - the folder containing all of a single session's data\n",
    "    ├   └── session_1.mkv    # depth data - the recording itself\n",
    "    ...\n",
    "    ├── session_n/ **\n",
    "    └── └── session_n.mkv\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  `progress.yaml` File\n",
    "\n",
    "This notebook generates a `progress.yaml` file that stores the filepaths to data generated from this notebook, including extraction data files, PC scores of the extractions and model results.\n",
    "\n",
    "If your notebook kernel is shutdown, you can load the progress file to 'restore' your progress. The progress file does **not** track MoSeq pipeline operations that were executed outside of this notebook (for example, if you were to run PCA using the command line interface). If necessary, you can manually modify the paths in the progress file or the corresponding `progress_paths` dictionary to access the output of these external operations.\n",
    "\n",
    "We recommend running this notebook from the folder where your data is located so the results are better organized. In that case, you can specify the `base_dir` as `./` (or the current folder). \n",
    "\n",
    "__To initialize a `progress.yaml` file or restore previous progress, run the following cell.__\n",
    "\n",
    "- A `progress.yaml` file will be generated if there is not such a file in the base directory. \n",
    "\n",
    "- If there is a `progress.yaml` file in the directory, the information will be loaded into the progress_paths dictionary. The `check_progress` function will print progress bars for each pipeline step in the notebook. The extraction progress bar indicates the total the number of extracted sessions detected in the provided `base_dir` path.\n",
    "- It prints the session names that haven't been extracted. __Note: the progress does not reflect the contents of the aggregate_results/ folder.__\n",
    "- The remainder of the progress bars are derived from reading the paths in the `progress_paths` dictionary, and the bars will fill up if the included paths are found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup/Restore Progress Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from moseq2_app.gui.progress import check_progress, restore_progress_vars\n",
    "\n",
    "# Add the path to your data folder here.\n",
    "# We recommend that you run this notebook in the same folder as your data. In that case, you don't have to change base_dir\n",
    "base_dir = './'\n",
    "progress_filepath = join(base_dir, 'progress.yaml')\n",
    "\n",
    "progress_paths = restore_progress_vars(progress_filepath, init=True, overwrite=False)\n",
    "check_progress(progress_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  `config.yaml` File\n",
    "\n",
    "The notebook generates a `config.yaml` that holds all configurable parameters for all steps in the MoSeq pipeline. Parameters will be added to this file as you progress through the notebook. The config file can be used to run an identical pipeline in future analyses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from moseq2_app.gui.progress import update_progress\n",
    "from moseq2_extract.gui import generate_config_command\n",
    "\n",
    "config_filepath = join(progress_paths['base_dir'], 'config.yaml')\n",
    "\n",
    "print(f'generating file in path: {config_filepath}')\n",
    "generate_config_command(config_filepath)\n",
    "progress_paths = update_progress(progress_filepath, 'config_file', config_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download a Pre-trained Flip Classifier Model for Orienting the Mouse\n",
    "\n",
    "MoSeq2 uses a Random Forest flip classifier to guarantee that the mouse's nosse is always pointed to the right after cropping and rotationally aligning the depth videos. The flip classifiers we provide __are trained for experiments run with C57BL/6 mice using with Kinect v2 depth cameras__.\n",
    "\n",
    "If your dataset does not work with our pre-trained flip classifiers, we provide a [flip-classifier training notebook](https://github.com/dattalab/moseq2-app/tree/jupyter/). After using this notebook, add the absolute path of your custom classifier to the `flip_classifier` field `config.yaml` file.\n",
    "\n",
    ":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_extract.gui import download_flip_command\n",
    "# selection=0 - large mice with fibers (default)\n",
    "# selection=1 - adult male C57s\n",
    "# selection=2 - mice with Inscopix cables\n",
    "download_flip_command(progress_paths['base_dir'], config_filepath, selection=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Data Extraction\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1XtDo6sVtvG0Grp5pDgLbFcli2_hRcTZK\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive ROI Detection Tool\n",
    "\n",
    "Use this interactive tool to optimize the extraction parameters prior to extracting all of your data. Most of the parameters are related to detecting the Region of Interest (ROI), which is the region occupied by the mouse. This tool can also be used to catch possibly corrupted or inconsistent sessions, and to diagnose ROI detection/extraction errors.\n",
    "\n",
    "### Usage\n",
    "- Run the following cell launch the ROI Detection Tool.\n",
    "- Click on a session from `Session Select` to load the session for interactive ROI detection.\n",
    "- Drag the slider or change the number on the right of the slider to change the parameters. Click away from the slider such that the handle change from blue to white to set the parameters.\n",
    "- Use the `Current Frame` slider to change the displayed preview session frame in the bottom 2 plots.\n",
    "- Use the `Frame Range to Extract` slider to select the range of frames to include in the preview video segment. Click `Extract Example` to extract the selected frames and display preview video.\n",
    "- **If the parameters for the session's ROI are acceptable, the dot next to the session name will turn green, indicating that the session is ready for extraction**. If the parameters for ROI hasn't been examined or aren't acceptable, the dot next to the session name will be red.\n",
    "- Click `Save ROI` if you want to save the current parameters and mark the session as passing, ignoring the warning.\n",
    "- Click `Check All Sessions` to check if the parameters for all the sessions are acceptable. If you have set parameters for specific sessions, those parameters will be check. Otherwise, the default and autodect parameters will be checked. A set of filters described [here](https://github.com/dattalab/moseq2-app/wiki/Analysis:-extraction#check-all-session-protocol) will be applied to detect possible poor extractions.\n",
    "- Click `Save Parameters` to save the parameters for extraction when all sessions pass.\n",
    "\n",
    "### Troubleshooting for Not Passing Sessions\n",
    "#### ROI Not Detected Correctly\n",
    "Adjust the Depth Range Selector to include the depth range of the detected bucket floor distance (which can be found by hovering over the Background image with your mouse). You can also manually enter slider values by clicking on the numbers.\n",
    "If the mouse seems to be cropped when at the bucket edge, increase the \"dilate iterations\" settings to enlarge the size of the included floor area.\n",
    "#### Too Much Noise Around Mouse Body\n",
    "Use the Rodent Height Threshold Slider to remove any noise/speckle from the bucket floor or walls. \n",
    "Ensure the min height parameter is small enough to only filter out floor reflections. Do not exclude too much of the mouse's body.\n",
    "Ensure the max height parameter is large enough to include the largest possible mouse height, (i.e., when the mouse is rearing). A reasonable value is around 100 mm for Kinect v2 recordings.\n",
    "Hover over the mouse in either of the bottom two plots to explore its height.\n",
    "#### Flagged Session\n",
    "If a session is flagged, click on the session and adjust the parameters until the session passes. Click `Save Parameters` to save the parameters. The `Save ROI` button can be used to manually accept a session's parameter set.\n",
    "\n",
    "__Note: if cell seems to be running out of memory after first use, set `compute_all_bgs=False` to reduce the memory pressure.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import ruamel.yaml as yaml\n",
    "from moseq2_app.gui.progress import update_progress\n",
    "from moseq2_app.main import interactive_roi_detector\n",
    "\n",
    "session_config_path = join(progress_paths['base_dir'], 'session_config.yaml')\n",
    "progress_paths = update_progress(progress_filepath, 'session_config', session_config_path)\n",
    "\n",
    "with open(progress_paths['config_file'], 'r') as f:\n",
    "    config_data = yaml.safe_load(f)\n",
    "\n",
    "config_data['camera_type'] = 'auto' # 'kinect', 'azure', or 'manual'\n",
    "config_data['crop_size'] = (80, 80)\n",
    "config_data['output_dir'] = 'proc' # the subfolder extracted data is saved to\n",
    "\n",
    "# increase this value to include a larger ROI region, and vice versa\n",
    "config_data['noise_tolerance'] = 30\n",
    "\n",
    "# OPTIONAL additional parameters\n",
    "# config_data['flip_classifier'] = './alternative-flip-classifier.pkl' # updated flip classifier path\n",
    "# config_data['spatial_filter_size'] = [3] # spatial filtering kernel size; must be odd\n",
    "# config_data['temporal_filter_size'] = (0,) # temporal filtering kernel size; must be odd\n",
    "\n",
    "# Filtering out head-fixed cables?\n",
    "# config_data['cable_filter_iters'] = 3 # number of cable filtering iterations\n",
    "# config_data['cable_filter_size'] = (7, 7) # cable spatial filter kernel size\n",
    "\n",
    "with open(progress_paths['config_file'], 'w') as f:\n",
    "    yaml.safe_dump(config_data, f)\n",
    "\n",
    "compute_all_bgs = True # If False, only computes the first background on launch\n",
    "                \n",
    "autodetect_depths = False # If True, will readjust the bg_depth_range for each session \n",
    "\n",
    "overwrite = False # if True, will overwrite the previously saved session_config.yaml file\n",
    "\n",
    "interactive_roi_detector(progress_paths, \n",
    "                         compute_all_bgs=compute_all_bgs, \n",
    "                         autodetect_depths=autodetect_depths, \n",
    "                         overwrite=overwrite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Session(s)\n",
    "Run the following cell to extract all the sessions.\n",
    "\n",
    "__Note: If sessions are not listed when running the cell, check whether the file extension of your depth files is included in the `extensions` variable.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from moseq2_extract.gui import extract_found_sessions\n",
    "\n",
    "# include the file extensions for the depth files you would like to search for and extract.\n",
    "extensions = ['.avi', '.dat'] # .avi, .dat, and/or .mkv\n",
    "\n",
    "# Set to False to select specific recordings to extract\n",
    "extract_all = True\n",
    "\n",
    "# Set to False to re-extract extracted recordings\n",
    "skip_extracted = True\n",
    "\n",
    "extract_found_sessions(progress_paths['base_dir'], \n",
    "                       progress_paths['config_file'], \n",
    "                       extensions, \n",
    "                       extract_all=extract_all, \n",
    "                       skip_extracted=skip_extracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Extraction Validation Tests (optional)\n",
    "\n",
    "Once all the extractions are complete, run the following cell to run data validation tests. The tests can output either an error or a warning. \n",
    "- An __error__ indicates that the session is corrupted in some way and should be excluded from PCA and Modeling.\n",
    "- A __warning__ indicates that one or more sessions are statistical outliers.\n",
    "  - A warning can indicate that the session may need to be inspected prior to continuing into the PCA step. \n",
    "  - Warnings can be ignored when they are consistent with experimental design (e.g. abnormally high velocity in an animal that recieved a stimulant drug). \n",
    "\n",
    "To diagnose certain scalar anomalies, use the [Scalar Summary](#Scalar-Summary-for-Further-Extraction-Diagnostics) below to graph any desired scalar value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_app.main import validate_extractions\n",
    "\n",
    "validate_extractions(progress_paths['base_dir']) # path to pre-existing aggregate_results/ folder is also permissible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Extraction Output (optional)\n",
    "\n",
    "Run the following cell to view the extraction output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_app.main import preview_extractions\n",
    "\n",
    "preview_extractions(progress_paths['base_dir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `moseq2-index.yaml` File\n",
    "The following cell will search for the `proc/` subfolders generated by extraction, and copy them to a single `aggregate_results/` folder. The notebook generates a `moseq2-index.yaml` from the metadata for all extracted sessions. The `aggregate_results/` folder contains all the data you need to run the rest of the pipeline. The PCA and modeling step will use data in this folder.\n",
    "\n",
    "**Important Note: The index file contains UUIDs to map each session to a specific extraction. If you want to re-extract session(s), delete the existing `moseq2-index.yaml` file and re-aggregate the extracted results to keep the `moseq2-index.yaml` updated.** Not doing so may cause `KeyError`s in the PCA and modeling steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate your extraction results and generate an index file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from moseq2_app.gui.progress import update_progress\n",
    "from moseq2_extract.gui import aggregate_extract_results_command\n",
    "\n",
    "recording_format = '{start_time}_{session_name}_{subject_name}' # filename formats for the copied extracted data files\n",
    "\n",
    "# directory NAME to save all metadata+extracted videos to with above respective name format\n",
    "aggregate_results_dirname = 'aggregate_results/'\n",
    "\n",
    "train_data_dir = join(progress_paths['base_dir'], aggregate_results_dirname)\n",
    "update_progress(progress_filepath, 'train_data_dir', train_data_dir)\n",
    "\n",
    "# the subpath indicates to only aggregate extracted session paths with that subpath, only change if aggregating data from a different location\n",
    "index_filepath = aggregate_extract_results_command(progress_paths['base_dir'], recording_format, aggregate_results_dirname)\n",
    "progress_paths = update_progress(progress_filepath, 'index_file', index_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Setter\n",
    "The `group` field in the `moseq2-index.yaml` is used to store group labels, so the sessions can by grouped by experimental design for downstream analysis. Group labels in the `moseq2-index.yaml` can be used analyses comparing different cohorts or experimental conditions. Initially, all sessions are labeled \"default\" and the Group Setter tool below is used to assigning group labels to sessions. This step requires that all your sessions have a metadata.json file containing a session name.\n",
    "\n",
    "Usage:\n",
    "- Run the following cell to launch the Group Setter tool.\n",
    "- Click on a column name to sort the table by values in the column.\n",
    "- Click the filter button to filter the values in a column.\n",
    "- Click on the session to select the session. To select multiple sessions, click the sessions while holding the CTRL/COMMAND key, or click the first and last entry while holding the SHIFT key.\n",
    "- Enter the group name in the text field and click `Set Group` update the `group` column for the selected sessions.\n",
    "- Click the `Update Index File` button to save current group assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_app.main import interactive_group_setting\n",
    "\n",
    "interactive_group_setting(progress_paths['index_file'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalar Summary for Further Extraction Diagnostics\n",
    "\n",
    "The following visualization tools could be used to discover sessions where the measured mouse sizes, speeds are possibly outliers. If outlier sessions do exist, refer to the following diagnostic steps:\n",
    "- Preview the extraction video using the [Preview Extractions tool](#Review-Extraction-Output-(optional)) and check for any irregularities that could indicate the session either needs to be re-extracted or discarded (due to different forms of corruption).\n",
    "- Run the [Position Heatmaps Cell](#Plot-Position-Heatmaps-For-Each-Session-(Optional)) to determine whether a session's ROI was correctly computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Scalar Summary\n",
    "Usage:\n",
    "- Run the following cell to plot a summary of scalar values for each group, such as average velocity, height, etc.\n",
    "- Hold CTRL/Command key and click on the item(s) Selector to select one or multiple scalars to plot.\n",
    "- Hover over the data points to display the session information.\n",
    "- Click on the legend items to show/hide groups from the plot. Double click an item to show a single group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_app.main import interactive_scalar_summary\n",
    "\n",
    "interactive_scalar_summary(progress_paths['index_file'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Position Heatmaps For Each Session\n",
    "Run the following cell to determine any ROI or position related anomalies that may exist within your extracted dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import matplotlib as mpl\n",
    "from moseq2_viz.gui import plot_verbose_position_heatmaps\n",
    "\n",
    "output_file = join(progress_paths['plot_path'], 'session_heatmaps') \n",
    "\n",
    "normalize = False\n",
    "\n",
    "norm_color = mpl.colors.LogNorm() # set to None for default normalization color scheme\n",
    "\n",
    "verbose_heatmap_fig = plot_verbose_position_heatmaps(progress_paths['index_file'], \n",
    "                                                     output_file,\n",
    "                                                     normalize=normalize,\n",
    "                                                     norm_color=norm_color\n",
    "                                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Group Mean Position Summary\n",
    "\n",
    "Run the following cell to get a general preview of each groups' environment exploration profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import matplotlib as mpl\n",
    "from moseq2_viz.gui import plot_mean_group_position_heatmaps_command\n",
    "\n",
    "output_file = join(progress_paths['plot_path'], 'group_heatmaps') \n",
    "\n",
    "normalize = False\n",
    "\n",
    "norm_color = mpl.colors.LogNorm() # set to None for default normalization color scheme\n",
    "\n",
    "group_mean_heatmap_fig = plot_mean_group_position_heatmaps_command(progress_paths['index_file'], \n",
    "                                                                   output_file,\n",
    "                                                                   normalize=normalize,\n",
    "                                                                   norm_color=norm_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting PCA\n",
    "\n",
    "Fit PCA to your extracted data to determine the principal components (PCs) that explain the largest possible variance in your dataset. The PCs should look smooth and well defined. The PCs should explain >90% of the variance in the dataset using around 10 PCs. If this isn't the case, please check out our [Wiki](https://github.com/dattalab/moseq2-app/wiki) for more information. If running PCA locally, progress can be monitored using the [dask server](https://localhost:8787/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import ruamel.yaml as yaml\n",
    "from moseq2_pca.gui import train_pca_command\n",
    "from moseq2_app.gui.progress import update_progress\n",
    "\n",
    "pca_filename = 'pca' # Name of your PCA model h5 file to be saved\n",
    "pca_dirname = join(progress_paths['base_dir'], '_pca/') # Directory to save your computed PCA results\n",
    "\n",
    "with open(progress_paths['config_file'], 'r') as f:\n",
    "    config_data = yaml.safe_load(f)\n",
    "\n",
    "# PCA parameters you may need to configure\n",
    "config_data['overwrite_pca'] = False\n",
    "config_data['gaussfilter_space'] = (1.5, 1) # Spatial filter for data (Gaussian)\n",
    "config_data['medfilter_space'] = [0] # Median spatial filter\n",
    "config_data['medfilter_time'] = [0] # Median temporal filter\n",
    "\n",
    "# If dataset includes head-attached cables, set missing_data=True\n",
    "config_data['missing_data'] = False # Set True for dataset with missing/dropped frames to reconstruct respective PCs.\n",
    "config_data['missing_data_iters'] = 10 # Number of times to iterate over missing data during PCA\n",
    "config_data['recon_pcs'] = 10 # Number of PCs to use for missing data reconstruction\n",
    "\n",
    "# Dask Configuration\n",
    "config_data['dask_port'] = '8787' # port to access Dask Dashboard\n",
    "\n",
    "# UNCOMMENT to use SLURM\n",
    "# config_data['cluster_type'] = 'slurm'\n",
    "# config_data['nworkers'] = 8 # number of spawned jobs\n",
    "# config_data['queue'] = 'short' # partition\n",
    "# config_data['memory'] = '40GB' # amount of memory per worker\n",
    "# config_data['cores'] = 1 # number of cores per worker\n",
    "# config_data['wall_time'] = '01:00:00' # worker time limit\n",
    "\n",
    "# UNCOMMENT if recordings contain occlusions (e.g. from overhead cables)\n",
    "# config_data['missing_data'] = True\n",
    "\n",
    "with open(progress_paths['config_file'], 'w') as f:\n",
    "    yaml.safe_dump(config_data, f)\n",
    "\n",
    "progress_paths = update_progress(progress_filepath, 'pca_dirname', pca_dirname)\n",
    "\n",
    "# will train on data in aggregate_results/\n",
    "train_pca_command(progress_paths, pca_dirname, pca_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from IPython.display import display, Image\n",
    "images = [join(progress_paths['pca_dirname'], 'pca_components.png'), \n",
    "          join(progress_paths['pca_dirname'], 'pca_scree.png')]\n",
    "for im in images:\n",
    "    display(Image(im))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Principal Component Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from moseq2_pca.gui import apply_pca_command\n",
    "from moseq2_app.gui.progress import update_progress\n",
    "\n",
    "scores_filename = 'pca_scores' # name of the scores file to compute and save\n",
    "\n",
    "scores_file = join(progress_paths['pca_dirname'], scores_filename + '.h5') # path to input PC scores file to model\n",
    "progress_paths = update_progress(progress_filepath, 'scores_path', scores_file)\n",
    "\n",
    "apply_pca_command(progress_paths, scores_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Model-Free Changepoints (Optional) \n",
    "\n",
    "This step can be used to determine a target syllable duration for the modeling step. Typically the distribution of change-point durations is smooth, left-skewed and peak around 0.3 seconds. If that is not the case, please check out our [Wiki](https://github.com/dattalab/moseq2-app/wiki) for more information.\n",
    "\n",
    "__Note: the parameters below are configured for C57 mouse data, and have not been tested for other strains/species.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ruamel.yaml as yaml\n",
    "from moseq2_app.gui.progress import update_progress\n",
    "from moseq2_pca.gui import compute_changepoints_command\n",
    "\n",
    "with open(progress_paths['config_file'], 'r') as f:\n",
    "    config_data = yaml.safe_load(f)\n",
    "\n",
    "changepoints_filename = 'changepoints' # name of the changepoints images to generate\n",
    "\n",
    "# Changepoint computation parameters you may want to configure\n",
    "config_data['threshold'] = 0.5 # Peak threshold to use for changepoints\n",
    "config_data['dims'] = 300 # Number of random projections to compare the computed principal components with\n",
    "\n",
    "with open(progress_paths['config_file'], 'w') as f:\n",
    "    yaml.safe_dump(config_data, f)\n",
    "\n",
    "progress_paths = update_progress(progress_filepath, 'changepoints_path', changepoints_filename)\n",
    "compute_changepoints_command(progress_paths['train_data_dir'], progress_paths, changepoints_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from IPython.display import display, Image\n",
    "\n",
    "changepoints_filename = 'changepoints'\n",
    "display(Image(join(progress_paths['pca_dirname'], changepoints_filename + '_dist.png')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARHMM Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the ARHMM\n",
    "\n",
    "Fitting the ARHMM typically requires adjusting the `kappa` hyperparameter to achieve a target syllable duration (higher values of `kappa` lead to longer syllable durations). The target duration can be determined using change-points analysis or set heuristically to 0.3-0.4 seconds based on prior literature. In the code below, set `kappa` to `'scan'` to run a family of models with different `kappa` values and use the \"Get Best Model Fit\" cell to pick an optimal value. We recommend fitting for 100-200 iterations to pick `kappa`. For final model fitting, set `kappa` to the chosen value and fit for ~1000 iterations. \n",
    "\n",
    "__Note: if loading a model checkpoint, ensure the modeling parameters (especially the selected groups) are identical to that of the checkpoint. Otherwise the model will fail.__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Model Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "\n",
    "# set model path\n",
    "modeling_session_path = 'saline-amphetamine/'\n",
    "model_name = 'model.p'\n",
    "\n",
    "session_path = join(progress_paths['base_dir'], modeling_session_path)\n",
    "model_path = join(session_path, model_name) # path to save trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model saving freqency (in interations); will create a checkpoints/ directory containing checkpointed models\n",
    "checkpoint_freq = -1\n",
    "use_checkpoint = False # resume training from latest saved checkpoint\n",
    "\n",
    "npcs = 10  # number of PCs being used; base case should be npcs should have explained variance >= ~90% \n",
    "max_states = 100 # number of maximum states the ARHMM can end up with\n",
    "\n",
    "# use robust-ARHMM with t-distribution -> able to tolerate more noise\n",
    "robust = True \n",
    "\n",
    "# separate group transition graphs; set to True if you want to compare multiple groups\n",
    "separate_trans = True \n",
    "\n",
    "num_iter = 100 # number of iterations to train model\n",
    "\n",
    "# syllable length probability distribution prior; (None, int or 'scan'); if None, kappa=nframes\n",
    "kappa = None \n",
    "\n",
    "select_groups = False # select specific groups to model; if False, will model all data as is in moseq2-index.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from moseq2_model.gui import learn_model_command\n",
    "from moseq2_app.gui.progress import update_progress\n",
    "\n",
    "### Advanced Modeling Parameters ###\n",
    "\n",
    "hold_out = False # boolean to hold out data subset during the training process\n",
    "nfolds = 2 # (if hold_out==True): number of folds to hold out during training; 1 fold per session\n",
    "\n",
    "# if kappa == 'scan', optionally set bounds to scan kappa values between, in either a linear or log-scale.\n",
    "scan_scale = 'log' # or linear\n",
    "min_kappa = None\n",
    "max_kappa = None\n",
    "out_script = 'train_out.sh' # script file to save kappa-scanning learn_model() commands \n",
    "\n",
    "# total number of models to spool\n",
    "n_models = 15\n",
    "\n",
    "# Select platform to run models on\n",
    "cluster_type = 'local' # currently supported cluster_types = 'local' or 'slurm'\n",
    "run_cmd = False # if True, runs the commands via os.system(...), script must be run manually otherwise\n",
    "\n",
    "## SLURM PARAMETERS\n",
    "## only edit these parameters if cluster_type == 'slurm'\n",
    "memory = '16GB'\n",
    "wall_time='3:00:00'\n",
    "partition='short'\n",
    "\n",
    "progress_paths = update_progress(progress_filepath, 'model_path', model_path)\n",
    "progress_paths = update_progress(progress_filepath, 'model_session_path', session_path)\n",
    "\n",
    "learn_model_command(progress_paths, hold_out=hold_out, nfolds=nfolds, num_iter=num_iter, max_states=max_states,\n",
    "                    npcs=npcs, kappa=kappa, separate_trans=separate_trans, robust=robust,\n",
    "                    checkpoint_freq=checkpoint_freq, use_checkpoint=use_checkpoint, select_groups=select_groups,\n",
    "                    cluster_type=cluster_type, min_kappa=min_kappa, scan_scale=scan_scale,\n",
    "                    max_kappa=max_kappa, n_models=n_models, run_cmd=run_cmd, output_dir=modeling_session_path,\n",
    "                    out_script=out_script, memory=memory, wall_time=wall_time, partition=partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Best Model Fit\n",
    "\n",
    "Use this feature to determine whether the trained model has captured median syllable durations that match the principal components changepoints.\n",
    "\n",
    "This feature can also return the best model from a list of models found in the `progress_paths['model_session_path']`.\n",
    "\n",
    "Below are examples of some comparative distributions that you can expect when using this tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from moseq2_viz.gui import get_best_fit_model\n",
    "from moseq2_app.gui.progress import update_progress\n",
    "\n",
    "output_file = join(progress_paths['plot_path'], 'model_vs_pc_changepoints')\n",
    "\n",
    "best_model_fit = get_best_fit_model(progress_paths, plot_all=True)\n",
    "progress_paths = update_progress(progress_filepath, 'model_path', best_model_fit['best model - duration'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Notebook End\n",
    "\n",
    "## Go to the [Interactive-Model-Results.ipynb](./Interactive-Model-Results-Exploration.ipynb) Jupyter Notebook to analyze model results."
   ]
  }
 ],
 "metadata": {
  "finalized": {
   "timestamp": 1623072349278,
   "trusted": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "303.984px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
