{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Moseq2-App\" data-toc-modified-id=\"Moseq2-App-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Moseq2 App</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Resources\" data-toc-modified-id=\"Resources-1.0.1\"><span class=\"toc-item-num\">1.0.1&nbsp;&nbsp;</span>Resources</a></span></li><li><span><a href=\"#Feedback\" data-toc-modified-id=\"Feedback-1.0.2\"><span class=\"toc-item-num\">1.0.2&nbsp;&nbsp;</span>Feedback</a></span></li><li><span><a href=\"#Data-Acquisition-Overview\" data-toc-modified-id=\"Data-Acquisition-Overview-1.0.3\"><span class=\"toc-item-num\">1.0.3&nbsp;&nbsp;</span>Data Acquisition Overview</a></span></li></ul></li><li><span><a href=\"#Notebook-Setup\" data-toc-modified-id=\"Notebook-Setup-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Notebook Setup</a></span><ul class=\"toc-item\"><li><span><a href=\"#Check-to-see-if-you're-running-python-from-the-correct-conda-enviornment\" data-toc-modified-id=\"Check-to-see-if-you're-running-python-from-the-correct-conda-enviornment-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Check to see if you're running python from the correct conda enviornment</a></span></li><li><span><a href=\"#Check-if-the-dependencies-are-found\" data-toc-modified-id=\"Check-if-the-dependencies-are-found-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Check if the dependencies are found</a></span></li><li><span><a href=\"#Alternative-Notebook-Setup\" data-toc-modified-id=\"Alternative-Notebook-Setup-1.1.3\"><span class=\"toc-item-num\">1.1.3&nbsp;&nbsp;</span>Alternative Notebook Setup</a></span></li></ul></li><li><span><a href=\"#Data-file-organization\" data-toc-modified-id=\"Data-file-organization-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Data file organization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Notebook-Progress-File\" data-toc-modified-id=\"Notebook-Progress-File-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Notebook Progress File</a></span></li><li><span><a href=\"#Restore-Progress-Variables\" data-toc-modified-id=\"Restore-Progress-Variables-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Restore Progress Variables</a></span></li><li><span><a href=\"#Generate-Configuration-Files\" data-toc-modified-id=\"Generate-Configuration-Files-1.2.3\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>Generate Configuration Files</a></span></li><li><span><a href=\"#Download-a-Flip-File\" data-toc-modified-id=\"Download-a-Flip-File-1.2.4\"><span class=\"toc-item-num\">1.2.4&nbsp;&nbsp;</span>Download a Flip File</a></span></li></ul></li></ul></li><li><span><a href=\"#Raw-Data-Extraction\" data-toc-modified-id=\"Raw-Data-Extraction-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Raw Data Extraction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Interactive-ROI-Detection-Tool\" data-toc-modified-id=\"Interactive-ROI-Detection-Tool-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Interactive ROI Detection Tool</a></span><ul class=\"toc-item\"><li><span><a href=\"#Basic-Usage\" data-toc-modified-id=\"Basic-Usage-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>Basic Usage</a></span></li><li><span><a href=\"#Troubleshooting\" data-toc-modified-id=\"Troubleshooting-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>Troubleshooting</a></span><ul class=\"toc-item\"><li><span><a href=\"#ROI-Not-Detected-Correctly\" data-toc-modified-id=\"ROI-Not-Detected-Correctly-2.1.2.1\"><span class=\"toc-item-num\">2.1.2.1&nbsp;&nbsp;</span>ROI Not Detected Correctly</a></span></li><li><span><a href=\"#Too-Much-Noise-Around-Mouse-Body\" data-toc-modified-id=\"Too-Much-Noise-Around-Mouse-Body-2.1.2.2\"><span class=\"toc-item-num\">2.1.2.2&nbsp;&nbsp;</span>Too Much Noise Around Mouse Body</a></span></li><li><span><a href=\"#Flagged-Session\" data-toc-modified-id=\"Flagged-Session-2.1.2.3\"><span class=\"toc-item-num\">2.1.2.3&nbsp;&nbsp;</span>Flagged Session</a></span></li></ul></li><li><span><a href=\"#Editable-Configuration-Parameter-Details\" data-toc-modified-id=\"Editable-Configuration-Parameter-Details-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>Editable Configuration Parameter Details</a></span></li><li><span><a href=\"#Restore-Progress-Variables\" data-toc-modified-id=\"Restore-Progress-Variables-2.1.4\"><span class=\"toc-item-num\">2.1.4&nbsp;&nbsp;</span>Restore Progress Variables</a></span></li></ul></li><li><span><a href=\"#Extract-Session(s)\" data-toc-modified-id=\"Extract-Session(s)-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Extract Session(s)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Run-Extraction-Validation-Tests-(optional)\" data-toc-modified-id=\"Run-Extraction-Validation-Tests-(optional)-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Run Extraction Validation Tests (optional)</a></span></li><li><span><a href=\"#Review-Extraction-Output-(optional)\" data-toc-modified-id=\"Review-Extraction-Output-(optional)-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Review Extraction Output (optional)</a></span></li></ul></li><li><span><a href=\"#Aggregate-your-results-into-one-folder-and-generate-an-index-file.\" data-toc-modified-id=\"Aggregate-your-results-into-one-folder-and-generate-an-index-file.-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Aggregate your results into one folder and generate an index file.</a></span></li><li><span><a href=\"#Specify-Groups\" data-toc-modified-id=\"Specify-Groups-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Specify Groups</a></span></li><li><span><a href=\"#Further-Extraction-Diagnostics\" data-toc-modified-id=\"Further-Extraction-Diagnostics-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Further Extraction Diagnostics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Compute-Scalar-Summary-(Diagnostics)\" data-toc-modified-id=\"Compute-Scalar-Summary-(Diagnostics)-2.5.1\"><span class=\"toc-item-num\">2.5.1&nbsp;&nbsp;</span>Compute Scalar Summary (Diagnostics)</a></span></li><li><span><a href=\"#Plot-Position-Heatmaps-For-Each-Session-(Diagnostics)\" data-toc-modified-id=\"Plot-Position-Heatmaps-For-Each-Session-(Diagnostics)-2.5.2\"><span class=\"toc-item-num\">2.5.2&nbsp;&nbsp;</span>Plot Position Heatmaps For Each Session (Diagnostics)</a></span></li><li><span><a href=\"#Plot-Group-Mean-Position-Summary-(Optional)\" data-toc-modified-id=\"Plot-Group-Mean-Position-Summary-(Optional)-2.5.3\"><span class=\"toc-item-num\">2.5.3&nbsp;&nbsp;</span>Plot Group Mean Position Summary (Optional)</a></span></li></ul></li></ul></li><li><span><a href=\"#Principal-Component-Analysis-(PCA)\" data-toc-modified-id=\"Principal-Component-Analysis-(PCA)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Principal Component Analysis (PCA)</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Restore-Progress-Variables\" data-toc-modified-id=\"Restore-Progress-Variables-3.0.1\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>Restore Progress Variables</a></span></li></ul></li><li><span><a href=\"#Fitting-PCA\" data-toc-modified-id=\"Fitting-PCA-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Fitting PCA</a></span></li><li><span><a href=\"#Computing-Principal-Component-Scores\" data-toc-modified-id=\"Computing-Principal-Component-Scores-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Computing Principal Component Scores</a></span><ul class=\"toc-item\"><li><span><a href=\"#Computing-Model-Free-Changepoints-(Optional)\" data-toc-modified-id=\"Computing-Model-Free-Changepoints-(Optional)-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Computing Model-Free Changepoints (Optional)</a></span></li></ul></li></ul></li><li><span><a href=\"#ARHMM-Modeling\" data-toc-modified-id=\"ARHMM-Modeling-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>ARHMM Modeling</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Restore-Progress-Variables\" data-toc-modified-id=\"Restore-Progress-Variables-4.0.1\"><span class=\"toc-item-num\">4.0.1&nbsp;&nbsp;</span>Restore Progress Variables</a></span></li></ul></li><li><span><a href=\"#Fitting-the-ARHMM\" data-toc-modified-id=\"Fitting-the-ARHMM-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Fitting the ARHMM</a></span><ul class=\"toc-item\"><li><span><a href=\"#Set-Model-Path\" data-toc-modified-id=\"Set-Model-Path-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Set Model Path</a></span></li><li><span><a href=\"#General-Parameters\" data-toc-modified-id=\"General-Parameters-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>General Parameters</a></span></li><li><span><a href=\"#Train-Model\" data-toc-modified-id=\"Train-Model-4.1.3\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span>Train Model</a></span></li><li><span><a href=\"#Restore-Notebook-Variables\" data-toc-modified-id=\"Restore-Notebook-Variables-4.1.4\"><span class=\"toc-item-num\">4.1.4&nbsp;&nbsp;</span>Restore Notebook Variables</a></span></li></ul></li><li><span><a href=\"#Get-Best-Model-Fit\" data-toc-modified-id=\"Get-Best-Model-Fit-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Get Best Model Fit</a></span></li></ul></li><li><span><a href=\"#Notebook-End\" data-toc-modified-id=\"Notebook-End-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Notebook End</a></span><ul class=\"toc-item\"><li><span><a href=\"#Go-to-the-Interactive-Model-Results.ipynb-Jupyter-Notebook-to-analyze-model-results.\" data-toc-modified-id=\"Go-to-the-Interactive-Model-Results.ipynb-Jupyter-Notebook-to-analyze-model-results.-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Go to the <a href=\"./Interactive-Model-Results-Exploration.ipynb\" target=\"_blank\">Interactive-Model-Results.ipynb</a> Jupyter Notebook to analyze model results.</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moseq2 App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1PxTnCMsrk3hRHPnEjqGDzq1oPkTYfzj0\">\n",
    "\n",
    "MoSeq2 is a software toolkit for unsupervised modeling and characterization of animal behavior. Moseq takes depth recordings of animals as input and outputs a rich description of behavior as a series of reused and stereotyped motifs called 'syllables'. \n",
    "\n",
    "This notebook begins with depth recordings (see the [data acquisition overview](#Data-Acquisition-Overview) below) and transforms this data through the steps of: \n",
    "\n",
    "- **[Extraction](#Raw-Data-Extraction)**: The animal is segmented from the background and its position and heading direction are aligned across frames.\n",
    "- **[Dimensionality reduction](#Principal-Component-Analysis-(PCA))**: Raw video is de-noised and transformed to low-dimensional pose trajectories using principal component analysis (PCA).\n",
    "- **[Model training](#ARHMM-Modeling)**: Pose trajectories are modeled using an autoregressive hidden Markov model (AR-HMM), producing a sequence of syllable labels.\n",
    "\n",
    "__The model output can be analyzed using the [Interactive Results Exploration](./Interactive-Model-Results-Exploration.ipynb) notebook.__\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Wiki](https://github.com/dattalab/moseq2-app/wiki) with instructions on data acquisition and command line options\n",
    "- PDF documentation of all MoSeq functions: [Extract](https://github.com/dattalab/moseq2-extract/blob/release/Documentation.pdf), [PCA](https://github.com/dattalab/moseq2-pca/blob/release/Documentation.pdf), [Model](https://github.com/dattalab/moseq2-model/blob/release/Documentation.pdf)\n",
    "- Publications:\n",
    "    - [Mapping Sub-Second Structure in Mouse Behavior](http://datta.hms.harvard.edu/wp-content/uploads/2018/01/pub_23.pdf)\n",
    "    - [The Striatum Organizes 3D Behavior via Moment-to-Moment Action Selection](http://datta.hms.harvard.edu/wp-content/uploads/2019/06/Markowitz.final_.pdf)\n",
    "    - [Revealing the structure of pharmacobehavioral space through motion sequencing](https://www.nature.com/articles/s41593-020-00706-3)\n",
    "    - [Q&A: Understanding the composition of behavior](http://datta.hms.harvard.edu/wp-content/uploads/2019/06/Datta-QA.pdf)\n",
    "    \n",
    "### Feedback\n",
    "\n",
    "For general feedback and feature requests, please fill out [this survey](https://forms.gle/FbtEN8E382y8jF3p6).\n",
    "    \n",
    "### Data Acquisition Overview\n",
    "MoSeq2 takes animal depth recordings as input. We we have developed a [data acquisition pipeline](https://github.com/dattalab/moseq2-app/wiki/Setup:-acquisition-software) for the second generation `Xbox Kinect` depth camera. We suggest following our [data acquisition tutorial](https://github.com/dattalab/moseq2-app/wiki/Acquisition) for doing recordings. MoSeq2 also accepts depth recordings from the `Azure Kinect` camera as well as the `Intel RealSense` using their standard data acquisitions pipelines.\n",
    "\n",
    "**We recommend recording more than 10 hours of depth video (~1 million frames at 30 frames per second) to ensure quality MoSeq models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1h2GYECyEuTMlM7Rx3Q3lMVBdWEm1F0S5\">\n",
    "\n",
    "### Check to see if you're running python from the correct conda enviornment\n",
    "\n",
    "If you performed the recommended installation, you should see the `sys.executable` path point to the python path within the `moseq2-app` environment, i.e.,\n",
    "```python\n",
    "import sys\n",
    "print(sys.executable)\n",
    "# /Users/username/miniconda3/envs/moseq2-app/bin/python\n",
    "```\n",
    "\n",
    "### Check if the dependencies are found\n",
    "\n",
    "Run the following cell to check if `moseq2-app` is installed in your current conda kernel. The latest working version number is `0.3.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!moseq2-extract --version\n",
    "!moseq2-pca --version\n",
    "!moseq2-model --version\n",
    "!moseq2-viz --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import moseq2_app\n",
    "\n",
    "print('Python path:', sys.executable)\n",
    "print('MoSeq2 app version:', moseq2_app.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Notebook Setup\n",
    "\n",
    "If your dataset has already been extracted, and the results have been aggregated into a singular folder, follow the pipeline described in the flow chart below in order to continue in the MoSeq analysis pipeline.\n",
    "\n",
    "Running the `Aggregate Results` cell with the data already existing in the path stored in the `aggregate_results_dirname` variable (by default `aggregate_results/`), will only generate an index file for the dataset. The index file is necessary for referencing all of the extractions' files throughout the analysis pipeline.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1BnID6Nu346AQPq5ShX_lNgt_U2Ts2-Cq\">\n",
    "\n",
    "## Data file organization\n",
    "\n",
    "The currently accepted depth data extensions are:\n",
    "- `.dat` (raw depth files from our kinect2 data acquisition software)\n",
    "- `.tar.gz` (compressed depth files from our kinect2 data acquisition software)\n",
    "- `.avi` (compressed depth files from the `moseq2-extract` command line interface)\n",
    "- `.mkv` (generated from Microsoft's recording software for the Azure Kinect)\n",
    "\n",
    "To run this notebook, create a master folder with a copy of this notebook, and a separate subfolders for each recording file (see example directory structure below). \n",
    "\n",
    "```\n",
    ".\n",
    "└── Data_Directory/\n",
    "    ├── Main-MoSeq2-Notebook.ipynb (running)\n",
    "    ├── session_1/ ** - the folder containing all of a single session's data\n",
    "    ├   ├── depth.dat        # depth data - the recording itself\n",
    "    ├   ├── depth_ts.txt     # timestamps - csv/txt file of the frame timestamps\n",
    "    ├   └── metadata.json    # metadata - json file that contains the rodent's info (group, subjectName, etc.)\n",
    "    ...\n",
    "    ├── session_2/ **\n",
    "    ├   ├── depth.dat\n",
    "    ├   ├── depth_ts.txt\n",
    "    └── └── metadata.json\n",
    "\n",
    "```\n",
    "\n",
    "__Note: if your data was acquired using an Azure Kinect or Intel RealSense depth camera, you will not have `depth_ts.txt` or `metadata.json` in your session directories. Before extraction you need to manually create a `metadata.json` file if you wish to identify sessions based on the session name or mouse ID.__ The metadata.json folder should minimally contain the following:\n",
    "\n",
    "```json\n",
    "{\"SessionName\": \"example session\", \"SubjectName\": \"example subject\", \"StartTime\": \"optional\"}\n",
    "```\n",
    "\n",
    "### Notebook Progress File\n",
    "\n",
    "This notebook generates a `progress.yaml` file that stores the filepaths to data generated from this notebook, including:\n",
    "- extraction data files\n",
    "- PC scores of the extractions\n",
    "- model results\n",
    "\n",
    "If your notebook kernel is shutdown, you can load the progress file to 'restore' your progress. The progress file does **not** track MoSeq pipeline operations that were executed outside of this notebook (for example, if you were to run PCA using the command line interface). If necessary, you can manually modify the paths in the progress file or the corresponding `progress_paths` dictionary to access the output of these external operations.\n",
    "\n",
    "__To restore previously computed variables, look for the cells following the `Restore Progress Variables` label.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore Progress Variables\n",
    "\n",
    "- Use this cell to load your notebook analysis progress. We recommend running this notebook from the folder where your data is located so the generated media will display properly. In that case, you can specify the `base_dir` as `./` (or the current folder).\n",
    "- The `base_dir` will be stored in the progress dict and will be reused throughout the notebooks.\n",
    "\n",
    "The `check_progress` function will print progress bars for each pipeline step in the notebook. \n",
    "- The extraction progress bar indicates total the number of extracted sessions detected in the provided `base_dir` path.\n",
    "- It prints the session names that haven't been extracted. __Note: the progress does not reflect the contents of the aggregate_results/ folder.__\n",
    "- The remainder of the progress bars are derived from reading the paths in the `progress_paths` dictionary, filling up the bar if the included paths are found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from moseq2_app.gui.progress import check_progress, restore_progress_vars\n",
    "\n",
    "# Add the path to your data folder here.\n",
    "# We recommend that you run this notebook in the same folder as your data. In that case, you don't have to change base_dir\n",
    "base_dir = './'\n",
    "progress_filepath = join(base_dir, 'progress.yaml')\n",
    "\n",
    "progress_paths = restore_progress_vars(progress_filepath, init=True, overwrite=False)\n",
    "check_progress(progress_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Configuration Files\n",
    "\n",
    "The `config.yaml` will be used to hold all configurable parameters for all steps in the MoSeq pipeline. Parameters will be added to this file as you progress through the notebook. The config file can be used to run an identical pipeline in future analyses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from moseq2_app.gui.progress import update_progress\n",
    "from moseq2_extract.gui import generate_config_command\n",
    "\n",
    "config_filepath = join(progress_paths['base_dir'], 'config.yaml')\n",
    "\n",
    "print(f'generating file in path: {config_filepath}')\n",
    "generate_config_command(config_filepath)\n",
    "progress_paths = update_progress(progress_filepath, 'config_file', config_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A configuration file has been created in the base directory (depicted below).\n",
    "\n",
    "```\n",
    ".\n",
    "└── Data_Directory/\n",
    "    ├── config.yaml **\n",
    "    ├── session_1/ \n",
    "    ├   ├── depth.dat        \n",
    "    ├   ├── depth_ts.txt     \n",
    "    ├   └── metadata.json    \n",
    "    ...\n",
    "    ├── session_2/ \n",
    "    ├   ├── depth.dat\n",
    "    ├   ├── depth_ts.txt\n",
    "    └── └── metadata.json\n",
    "```\n",
    "\n",
    "### Download a Flip File\n",
    "\n",
    "MoSeq2 uses a Random Forest flip classifier to guarantee that the mouse is always pointed to the right after cropping and rotationally aligning the depth videos. The flip classifiers we provide __are trained for experiments run with C57BL/6 mice using with Kinect v2 depth cameras__.\n",
    "\n",
    "If your dataset does not work with our pre-trained flip classifiers, we provide a [flip-classifier training notebook](https://github.com/dattalab/moseq2-app/tree/jupyter/). After using this notebook, add the path of your custom classifier to the `config.yaml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_extract.gui import download_flip_command\n",
    "# selection=0 - large mice with fibers (default)\n",
    "# selection=1 - adult male C57s\n",
    "# selection=2 - mice with Inscopix cables\n",
    "download_flip_command(progress_paths['base_dir'], config_filepath, selection=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Data Extraction\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1XtDo6sVtvG0Grp5pDgLbFcli2_hRcTZK\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive ROI Detection Tool\n",
    "\n",
    "Use this interactive tool to optimized the extraction parameters prior to extracting all of your data. Most of the parameters are related to detecting the region the mouse occupies (the ROI). This tool can also be used to catch possibly corrupted or inconsistent sessions, and to diagnose ROI detection/extraction errors.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td width=\"45%\">\n",
    "            <h3 style=\"text-align:left;\">Basic Usage</h3>\n",
    "            <ol>\n",
    "                <li style=\"text-align:left; font-size:14px\">\n",
    "                    Execute the code below to launch the ROI Detection Tool.\n",
    "                </li>\n",
    "                <li style=\"text-align:left; font-size:14px\">\n",
    "                    Click on any row in the session selector to load that session's data to the view.\n",
    "                </li>\n",
    "                <li style=\"text-align:left; font-size:14px\">\n",
    "                    If the indicator next to the session's name is green, then the session is considered ready for extraction. A red indicator can either mean the session has not been checked yet, or its extraction parameter set is incorrect.\n",
    "                </li>\n",
    "                <li style=\"text-align:left; font-size:14px\">\n",
    "                    Use the \"current frame\" slider to change the displayed session frame in the bottom 2 plots.\n",
    "                </li>\n",
    "                <li style=\"text-align:left; font-size:14px\">\n",
    "                    Change the frame range slider values to adjust the segments of the video to extract, then click the \"Extract Sample\" button to trigger an extraction and view the results.\n",
    "                </li>\n",
    "                <li style=\"text-align:left; font-size:14px\">\n",
    "                    Once you have found a satisfactory set of parameters, click \"Check All Sessions\" to test the parameters on all sessions. A set of filters described <a href=\"https://github.com/dattalab/moseq2-app/wiki/Analysis:-extraction#check-all-session-protocol\">here</a> will be applied to detect possible poor extractions.\n",
    "                </li>\n",
    "                <li style=\"text-align:left; font-size:14px\">\n",
    "                    If no sessions are flagged, click \"Save Parameters\". The parameters for each session will be written in the overall project config file and in each session-specific config file. \n",
    "                </li>\n",
    "                <li style=\"text-align:left; font-size:14px\">\n",
    "                    Clear the view and proceed to extracting all the data.\n",
    "                </li>\n",
    "            </ol>\n",
    "            <h3 style=\"text-align:left;\">Troubleshooting</h3>\n",
    "            <h4 style=\"text-align:left;\">ROI Not Detected Correctly</h4>\n",
    "            <ol>\n",
    "                <li style=\"text-align:left; font-size:14px\">\n",
    "                    Adjust the Depth Range Selector to include the depth range of the detected bucket floor distance (which can be found by hovering over the Background image with your mouse). You can also manually enter slider values by clicking on the numbers.\n",
    "                </li>\n",
    "                <li style=\"text-align:left; font-size:14px\">\n",
    "                    If the mouse seems to be cropped when at the bucket edge, increase the \"dilate iterations\" settings to enlarge the size of the included floor area.\n",
    "                </li>\n",
    "                </ol>\n",
    "            <h4 style=\"text-align:left;\">Too Much Noise Around Mouse Body</h4>\n",
    "                <ol>\n",
    "                <li style=\"text-align:left; font-size:14px\">\n",
    "                    Use the Rodent Height Threshold Slider to remove any noise/speckle from the bucket floor or walls. \n",
    "                    <ul>\n",
    "                        <li style=\"text-align:left; font-size:14px\">\n",
    "                            Ensure the min height parameter is small enough to only filter out floor reflections. Do not exclude too much of the mouse's body.\n",
    "                        </li>\n",
    "                        <li style=\"text-align:left; font-size:14px\">\n",
    "                            Ensure the max height parameter is large enough to include the largest possible mouse height, (i.e., when the mouse is rearing). A reasonable value is around 100 mm for Kinect v2 recordings.\n",
    "                        </li>\n",
    "                        <li style=\"text-align:left; font-size:14px\">\n",
    "                            Hover over the mouse in either of the bottom two plots to explore its height.\n",
    "                        </li>\n",
    "                    </ul>\n",
    "                </li>\n",
    "                </ol>\n",
    "            <h4 style=\"text-align:left;\">Flagged Session</h4>\n",
    "            <ol>\n",
    "                <li style=\"text-align:left; font-size:14px\">\n",
    "                    If a session is flagged, click on it in the Session Selector and a text indicator will appear with error details. Readjust the parameters until the session passes and then save the parameters. The \"Mark Passing\" button can be used to manually accept a session's parameter set.\n",
    "                </li>\n",
    "            </ol>\n",
    "            <p style=\"text-align:left; font-size:14px\">\n",
    "        </td>\n",
    "        <td width='55%'>\n",
    "            <img src=\"https://drive.google.com/uc?export=view&id=1iIj92Wl0Uezn_ehjvGnwV2YzGp8Pir6f\">\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td colspan=2>\n",
    "        <ul>\n",
    "            <li style=\"text-align:left; font-size:16px\">\n",
    "                If a session's parameters were individually/manually edited, then they will be preserved in the current state of the tool. Otherwise, they will be set to the initial/pre-existing options found in the config.yaml file.\n",
    "            </li>\n",
    "            <li style=\"text-align:left; font-size:16px\">\n",
    "                You can reload a session's edited parameters by revisiting the session using the Session Selector.\n",
    "            </li>\n",
    "            <li style=\"text-align:left; font-size:16px\">\n",
    "                Use the \"Check all Sessions\" button to test out whether your manually edited and/or autodetected depths yield passing ROI sizes. It will list out the number of checked and passing sessions.\n",
    "            </li>\n",
    "            <li style=\"text-align:left; font-size:16px\">\n",
    "                Finally, click the Save Parameters button once you are confident that the selected/edited will yield good extractions.\n",
    "            </li>\n",
    "        </ul>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "__Note: if cell seems to be running out of memory after first use, set `compute_all_bgs=False` to reduce the memory pressure.__\n",
    "\n",
    "### Editable Configuration Parameter Details\n",
    "Click here to view a table with descriptions for all the relevant `config_data` parameters you may change going into the extract step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import ruamel.yaml as yaml\n",
    "from moseq2_app.gui.progress import update_progress\n",
    "from moseq2_app.main import interactive_roi_detector\n",
    "\n",
    "session_config_path = join(progress_paths['base_dir'], 'session_config.yaml')\n",
    "progress_paths = update_progress(progress_filepath, 'session_config', session_config_path)\n",
    "\n",
    "with open(progress_paths['config_file'], 'r') as f:\n",
    "    config_data = yaml.safe_load(f)\n",
    "\n",
    "config_data['camera_type'] = 'auto' # 'kinect', 'azure', 'realsense', or 'manual'\n",
    "config_data['crop_size'] = (80, 80)\n",
    "config_data['output_dir'] = 'proc' # the subfolder extracted data is saved to\n",
    "\n",
    "# increase this value to include a larger ROI region, and vice versa\n",
    "config_data['noise_tolerance'] = 30\n",
    "\n",
    "# OPTIONAL additional parameters\n",
    "# config_data['flip_classifier'] = './alternative-flip-classifier.pkl' # updated flip classifier path\n",
    "# config_data['spatial_filter_size'] = [3] # spatial filtering kernel size; must be odd\n",
    "# config_data['temporal_filter_size'] = (0,) # temporal filtering kernel size; must be odd\n",
    "\n",
    "# Filtering out head-fixed cables?\n",
    "# config_data['cable_filter_iters'] = 3 # number of cable filtering iterations\n",
    "# config_data['cable_filter_size'] = (7, 7) # cable spatial filter kernel size\n",
    "\n",
    "with open(progress_paths['config_file'], 'w') as f:\n",
    "    yaml.safe_dump(config_data, f)\n",
    "\n",
    "compute_all_bgs = True # If False, only computes the first background on launch\n",
    "                \n",
    "autodetect_depths = False # If True, will readjust the bg_depth_range for each session \n",
    "\n",
    "overwrite = False # if True, will overwrite the previously saved session_config.yaml file\n",
    "\n",
    "interactive_roi_detector(progress_paths, \n",
    "                         compute_all_bgs=compute_all_bgs, \n",
    "                         autodetect_depths=autodetect_depths, \n",
    "                         overwrite=overwrite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Session(s)\n",
    "\n",
    "__Note: If sessions are not listed when running the cell, ensure your selected extension matches that of your depth files.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from moseq2_extract.gui import extract_found_sessions\n",
    "\n",
    "# include the file extensions for the depth files you would like to search for and extract.\n",
    "extensions = ['.avi', '.dat'] # .avi, .dat, and/or .mkv\n",
    "\n",
    "# Set to False to select specific recordings to extract\n",
    "extract_all = True\n",
    "\n",
    "# Set to False to re-extract extracted recordings\n",
    "skip_extracted = True\n",
    "\n",
    "extract_found_sessions(progress_paths['base_dir'], \n",
    "                       progress_paths['config_file'], \n",
    "                       extensions, \n",
    "                       extract_all=extract_all, \n",
    "                       skip_extracted=skip_extracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what your directory structure should look like once the process is complete:\n",
    "\n",
    "```\n",
    ".\n",
    "├── config.yaml\n",
    "├── session_1/\n",
    "├   ...\n",
    "├   └── proc/ **\n",
    "├   ├   ├── roi.tiff\n",
    "├   ├   ...\n",
    "├   ├   ├── results_00.yaml ** (.yaml file storing extraction parameters)\n",
    "├   ├   ├── results_00.h5 ** (.h5 file storing extraction)\n",
    "├   └   └── results_00.mp4 ** (extracted video)\n",
    "└── session_2/\n",
    "├   ...\n",
    "├   └── proc/ **\n",
    "├   ├   ├── roi.tiff\n",
    "├   ├   ...\n",
    "├   ├   ├── results_00.yaml **\n",
    "├   ├   ├── results_00.h5 **\n",
    "└   └   └── results_00.mp4 **\n",
    "        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Extraction Validation Tests (optional)\n",
    "\n",
    "Once all the extractions are complete, use the following cell to run data validation tests. The tests can output either an error or a warning. \n",
    "- An __error__ indicates that the session is corrupted in some way and should be excluded from PCA and Modeling.\n",
    "- A __warning__ indicates that one or more sessions are statistical outliers.\n",
    "  - A warning can indicate that the session may need to be inspected prior to continuing into the PCA step. \n",
    "  - Warnings can be ignored when they are consistent with experimental design (e.g. abnormally high velocity in an animal that recieved a stimulant drug). \n",
    "\n",
    "__Error tests__: \n",
    "- Count Dropped Frames: an error is raised if a session is missing >5% of the frames based on timestamps (requires a timestamp file).\n",
    "- Missing Mouse Check: raises an error if a mouse is missing from the video for any reason for >5% of the session's total frames.\n",
    "- Scalar Anomaly: raises an error if >5% of a session's computed scalar values are NaN.\n",
    "\n",
    "__Warning tests__:\n",
    "- Size Anomaly: Warning is raised when a mouse's captured body size is less than 2 standard deviations from the mean size throughout the session.\n",
    "- Scalar Anomaly: Warning is raised for a session if the trained [EllipticEnvelope](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.EllipticEnvelope.html) model classifies it as an outlier. \n",
    "- Position Anomaly: There are two cases that raise warnings:\n",
    "    1. Mouse is stationary for >5% of the session.\n",
    "    2. Mouse's position distribution is at least 2 standard deviations away from the mean of all the sessions, measured using Kullback–Leibler divergence.\n",
    "     - This anomaly can indicate that a mouse has explored a much larger or smaller region of the arena compared to the other recordings in the dataset.\n",
    "\n",
    "To diagnose certain scalar anomalies, use the [Scalar Summary Cell](#Compute-Scalar-Summary) below to graph any desired scalar value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_app.main import validate_extractions\n",
    "\n",
    "validate_extractions(progress_paths['base_dir']) # path to pre-existing aggregate_results/ folder is also permissible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Extraction Output (optional)\n",
    "\n",
    "Run the following cell to view the extraction output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_app.main import preview_extractions\n",
    "\n",
    "preview_extractions(progress_paths['base_dir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate your results into one folder and generate an index file.\n",
    "\n",
    "The following cell will search for the `proc/` subfolders containing the extraction output, and copy them to a single `aggregate_results/` folder. An index file called `moseq2-index.yaml` will also be generated with metadata for all extracted sessions. The index file can be used to group recordings by experimental condition for downstream analysis. \n",
    "Initially, each session as assigned to a single group called \"default\". We provide an interface for re-assigning group labels below.\n",
    "\n",
    "The `aggregate_results/` folder contains all the data you need to run the rest of the pipeline. The PCA and modeling step will use data in this folder.\n",
    "\n",
    "__Important Note: The index file contains UUIDs to map each session to a specific extraction. These UUIDs are referenced throughout the pipeline, so if you re-extract a session and re-aggregate your data, ensure all the UUIDs in the index file are up-to-date BEFORE running the PCA step.__ Not updating the index file will likely cause `KeyError`s to occur in the PCA and modeling steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from moseq2_app.gui.progress import update_progress\n",
    "from moseq2_extract.gui import aggregate_extract_results_command\n",
    "\n",
    "recording_format = '{start_time}_{session_name}_{subject_name}' # filename formats for the copied extracted data files\n",
    "\n",
    "# directory NAME to save all metadata+extracted videos to with above respective name format\n",
    "aggregate_results_dirname = 'aggregate_results/'\n",
    "\n",
    "train_data_dir = join(progress_paths['base_dir'], aggregate_results_dirname)\n",
    "update_progress(progress_filepath, 'train_data_dir', train_data_dir)\n",
    "\n",
    "# the subpath indicates to only aggregate extracted session paths with that subpath, only change if aggregating data from a different location\n",
    "index_filepath = aggregate_extract_results_command(progress_paths['base_dir'], recording_format, aggregate_results_dirname)\n",
    "progress_paths = update_progress(progress_filepath, 'index_file', index_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aggregate results folder will be saved in your base directory, e.g.\n",
    "\n",
    "```\n",
    ".\n",
    "├── aggregate_results/ **\n",
    "├   ├── session_1_results_00.h5 ** # session 1 compressed extraction + metadata \n",
    "├   ├── session_1_results_00.yaml **\n",
    "├   ├── session_1_results_00.mp4 ** # session 1 extracted video\n",
    "├   ├── session_2_results_00.h5 ** # session 2 compressed extraction + metadata \n",
    "├   ├── session_2_results_00.yaml **\n",
    "├   └── session_2_results_00.mp4 ** # session 2 extracted video\n",
    "├── config.yaml\n",
    "├── moseq2-index.yaml ** # index file\n",
    "├── session_1/\n",
    "└── session_2/\n",
    "```\n",
    "\n",
    "## Specify Groups\n",
    "Sessions can be given \"group\" labels in the `moseq2-index.yaml` for analyses comparing different cohorts or experimental conditions. This step requires that all your sessions have a metadata.json file containing a session name. Run the cell below to launch the group assignment GUI\n",
    "\n",
    "- Click on the column names to sort the index file (6).\n",
    "    - To filter the list for certain column values, click on the Filter Button (5) on the column you would like to filter.\n",
    "    - Next, enter the columm values you would like to display in the pop-up text box, and select all of the listed contents. (You can do that by clicking the first entry, then __while holding the SHIFT key__, clicking the last entry in the list.\n",
    "- Enter your desired group name in the text input (2) and click `Set Group` (3) to update all the associated session rows.\n",
    "- Once all your groups are set, click the `Update Index File` button (4) to save current group assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_app.main import interactive_group_setting\n",
    "\n",
    "interactive_group_setting(progress_paths['index_file'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Extraction Diagnostics\n",
    "\n",
    "- Use the following visualization tools to discover sessions where the measured mouse sizes, speeds are possibly out of the ordinary. If outlier sessions do exist, refer to the following diagnostic steps:\n",
    "    1. Preview the extraction video using the [Preview Extractions tool](#Review-Extraction-Output-(optional)) and check for any irregularities that could indicate the session either needs to be re-extracted or discarded (due to different forms of corruption).\n",
    "    2. To determine whether a session's ROI was correctly computed, run the [Position Heatmaps Cell](#Plot-Position-Heatmaps-For-Each-Session-(Optional)). \n",
    "\n",
    "### Compute Scalar Summary (Diagnostics)\n",
    "\n",
    "Use the following command to plot a summary of scalar values for each group, such as average velocity, height, etc.\n",
    "- Hold [CTRL]/[Command] and click on the Selector rows to select multiple scalars to plot.\n",
    "- Hover over any of the data points to display the session information.\n",
    "- Click on the legend items to show/hide groups from the plot. Double click an item to only show a single group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_app.main import interactive_scalar_summary\n",
    "\n",
    "interactive_scalar_summary(progress_paths['index_file'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Position Heatmaps For Each Session (Diagnostics)\n",
    "Use this cell to determine any ROI or position related anomalies that may exist within your extracted dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import matplotlib as mpl\n",
    "from moseq2_viz.gui import plot_verbose_position_heatmaps\n",
    "\n",
    "output_file = join(progress_paths['plot_path'], 'session_heatmaps') \n",
    "\n",
    "normalize = False\n",
    "\n",
    "norm_color = mpl.colors.LogNorm() # set to None for default normalization color scheme\n",
    "\n",
    "verbose_heatmap_fig = plot_verbose_position_heatmaps(progress_paths['index_file'], \n",
    "                                                     output_file,\n",
    "                                                     normalize=normalize,\n",
    "                                                     norm_color=norm_color\n",
    "                                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Group Mean Position Summary (Optional)\n",
    "\n",
    "Use this cell to get a general preview of each groups' environment exploration profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import matplotlib as mpl\n",
    "from moseq2_viz.gui import plot_mean_group_position_heatmaps_command\n",
    "\n",
    "output_file = join(progress_paths['plot_path'], 'group_heatmaps') \n",
    "\n",
    "normalize = False\n",
    "\n",
    "norm_color = mpl.colors.LogNorm() # set to None for default normalization color scheme\n",
    "\n",
    "group_mean_heatmap_fig = plot_mean_group_position_heatmaps_command(progress_paths['index_file'], \n",
    "                                                                   output_file,\n",
    "                                                                   normalize=normalize,\n",
    "                                                                   norm_color=norm_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting PCA\n",
    "\n",
    "Fit PCA to your extracted data to determine the principal components (PCs) that explain the largest possible variance in your dataset. The PCs should look smooth and well defined like the examples below. The PCs should explain >90% of the variance in the dataset using around 10 PCs. If this isn't the case, consult the [table of possible pathologies ](#Possible-PCA-Pathologies). If running PCA locally, progress can be monitored using the [dask server](https://localhost:8787/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import ruamel.yaml as yaml\n",
    "from moseq2_pca.gui import train_pca_command\n",
    "from moseq2_app.gui.progress import update_progress\n",
    "\n",
    "pca_filename = 'pca' # Name of your PCA model h5 file to be saved\n",
    "pca_dirname = join(progress_paths['base_dir'], '_pca/') # Directory to save your computed PCA results\n",
    "\n",
    "with open(progress_paths['config_file'], 'r') as f:\n",
    "    config_data = yaml.safe_load(f)\n",
    "\n",
    "# PCA parameters you may need to configure\n",
    "config_data['overwrite_pca'] = False\n",
    "config_data['gaussfilter_space'] = (1.5, 1) # Spatial filter for data (Gaussian)\n",
    "config_data['medfilter_space'] = [0] # Median spatial filter\n",
    "config_data['medfilter_time'] = [0] # Median temporal filter\n",
    "\n",
    "# If dataset includes head-attached cables, set missing_data=True\n",
    "config_data['missing_data'] = False # Set True for dataset with missing/dropped frames to reconstruct respective PCs.\n",
    "config_data['missing_data_iters'] = 10 # Number of times to iterate over missing data during PCA\n",
    "config_data['recon_pcs'] = 10 # Number of PCs to use for missing data reconstruction\n",
    "\n",
    "# Dask Configuration\n",
    "config_data['dask_port'] = '8787' # port to access Dask Dashboard\n",
    "\n",
    "# UNCOMMENT to use SLURM\n",
    "# config_data['cluster_type'] = 'slurm'\n",
    "# config_data['nworkers'] = 8 # number of spawned jobs\n",
    "# config_data['queue'] = 'short' # partition\n",
    "# config_data['memory'] = '40GB' # amount of memory per worker\n",
    "# config_data['cores'] = 1 # number of cores per worker\n",
    "# config_data['wall_time'] = '01:00:00' # worker time limit\n",
    "\n",
    "# UNCOMMENT if recordings contain occlusions (e.g. from overhead cables)\n",
    "# config_data['missing_data'] = True\n",
    "\n",
    "with open(progress_paths['config_file'], 'w') as f:\n",
    "    yaml.safe_dump(config_data, f)\n",
    "\n",
    "progress_paths = update_progress(progress_filepath, 'pca_dirname', pca_dirname)\n",
    "\n",
    "# will train on data in aggregate_results/\n",
    "train_pca_command(progress_paths, pca_dirname, pca_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once complete, a new directory `_pca` will be created containing the following data:\n",
    "```\n",
    ".\n",
    "├── _pca/ **\n",
    "├   ├── pca.h5 ** # pca model compressed file\n",
    "├   ├── pca.yaml  ** # pca model YAML metadata file\n",
    "├   ├── pca_components.png **\n",
    "├   └── pca_scree.png **\n",
    "├── aggregate_results/\n",
    "├── config.yaml\n",
    "├── moseq2-index.yaml\n",
    "├── session_1/\n",
    "└── session_2/\n",
    "\n",
    "```\n",
    "\n",
    "View your `computed PCs` and `scree plot` in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from IPython.display import display, Image\n",
    "images = [join(progress_paths['pca_dirname'], 'pca_components.png'), \n",
    "          join(progress_paths['pca_dirname'], 'pca_scree.png')]\n",
    "for im in images:\n",
    "    display(Image(im))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Principal Component Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from moseq2_pca.gui import apply_pca_command\n",
    "from moseq2_app.gui.progress import update_progress\n",
    "\n",
    "scores_filename = 'pca_scores' # name of the scores file to compute and save\n",
    "\n",
    "scores_file = join(progress_paths['pca_dirname'], scores_filename + '.h5') # path to input PC scores file to model\n",
    "progress_paths = update_progress(progress_filepath, 'scores_path', scores_file)\n",
    "\n",
    "apply_pca_command(progress_paths, scores_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output if this step is saved in a file called ```pca_scores.h5``` in the  ```_pca directory```. \n",
    "```\n",
    ".\n",
    "├── _pca/\n",
    "├   ├── pca.h5\n",
    "├   ├── pca.yaml\n",
    "├   ├── pca_scores.h5  ** # scores file\n",
    "├   ├── pca_components.png\n",
    "├   └── pca_scree.png\n",
    "├── aggregate_results/\n",
    "├── config.yaml\n",
    "├── moseq2-index.yaml\n",
    "├── session_1/\n",
    "└── session_2/\n",
    "\n",
    "```\n",
    "\n",
    "### Computing Model-Free Changepoints (Optional) \n",
    "\n",
    "This step can be used to determine a target syllable duration for the modeling step. Typically the distribution of change-point durations is smooth, left-skewed and centered around 0.3 seconds. If that is not the case, consult the [table of possible pathologies](#Possible-Model-Free-Changepoints-Pathologies) below.\n",
    "\n",
    "__Note: the parameters below are configured for C57 mouse data, and have not been tested for other strains/species.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ruamel.yaml as yaml\n",
    "from moseq2_app.gui.progress import update_progress\n",
    "from moseq2_pca.gui import compute_changepoints_command\n",
    "\n",
    "with open(progress_paths['config_file'], 'r') as f:\n",
    "    config_data = yaml.safe_load(f)\n",
    "\n",
    "changepoints_filename = 'changepoints' # name of the changepoints images to generate\n",
    "\n",
    "# Changepoint computation parameters you may want to configure\n",
    "config_data['threshold'] = 0.5 # Peak threshold to use for changepoints\n",
    "config_data['dims'] = 300 # Number of random projections to compare the computed principal components with\n",
    "\n",
    "with open(progress_paths['config_file'], 'w') as f:\n",
    "    yaml.safe_dump(config_data, f)\n",
    "\n",
    "progress_paths = update_progress(progress_filepath, 'changepoints_path', changepoints_filename)\n",
    "compute_changepoints_command(progress_paths['train_data_dir'], progress_paths, changepoints_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The changepoints plot will be generated and saved in the ```_pca``` directory.\n",
    "```\n",
    ".\n",
    "├── _pca/ \n",
    "├   ├── pca.h5\n",
    "├   ├── pca_scores.h5\n",
    "├   ...\n",
    "├   └── changepoints_dist.png **\n",
    "├── aggregate_results/ \n",
    "├── config.yaml\n",
    "├── moseq2-index.yaml\n",
    "├── session_1/\n",
    "└── session_2/\n",
    "```\n",
    "\n",
    "View ```changepoints_dist.png``` using the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from IPython.display import display, Image\n",
    "\n",
    "changepoints_filename = 'changepoints'\n",
    "display(Image(join(progress_paths['pca_dirname'], changepoints_filename + '_dist.png')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARHMM Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the ARHMM\n",
    "\n",
    "Fitting the ARHMM typically requires adjusting the `kappa` hyperparameter to achieve a target syllable duration (higher values of `kappa` lead to longer syllable durations). The target duration can be determined using change-points analysis or set heuristically to 0.3-0.4 seconds based on prior literature. In the code below, set `kappa` to `'scan'` to run a family of models with different `kappa` values and use the \"Get Best Model Fit\" cell to pick an optimal value. We recommend fitting for 100-200 iterations to pick `kappa`. For final model fitting, set `kappa` to the chosen value and fit for ~1000 iterations. \n",
    "\n",
    "__Note: if loading a model checkpoint, ensure the modeling parameters (especially the selected groups) are identical to that of the checkpoint. Otherwise the model will fail.__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Model Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "\n",
    "# set model path\n",
    "modeling_session_path = 'saline-amphetamine/'\n",
    "model_name = 'model.p'\n",
    "\n",
    "session_path = join(progress_paths['base_dir'], modeling_session_path)\n",
    "model_path = join(session_path, model_name) # path to save trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model saving freqency (in interations); will create a checkpoints/ directory containing checkpointed models\n",
    "checkpoint_freq = -1\n",
    "use_checkpoint = False # resume training from latest saved checkpoint\n",
    "\n",
    "npcs = 10  # number of PCs being used; base case should be npcs should have explained variance >= ~90% \n",
    "max_states = 100 # number of maximum states the ARHMM can end up with\n",
    "\n",
    "# use robust-ARHMM with t-distribution -> able to tolerate more noise\n",
    "robust = True \n",
    "\n",
    "# separate group transition graphs; set to True if you want to compare multiple groups\n",
    "separate_trans = True \n",
    "\n",
    "num_iter = 100 # number of iterations to train model\n",
    "\n",
    "# syllable length probability distribution prior; (None, int or 'scan'); if None, kappa=nframes\n",
    "kappa = None \n",
    "\n",
    "select_groups = False # select specific groups to model; if False, will model all data as is in moseq2-index.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from moseq2_model.gui import learn_model_command\n",
    "from moseq2_app.gui.progress import update_progress\n",
    "\n",
    "### Advanced Modeling Parameters ###\n",
    "\n",
    "hold_out = False # boolean to hold out data subset during the training process\n",
    "nfolds = 2 # (if hold_out==True): number of folds to hold out during training; 1 fold per session\n",
    "\n",
    "# if kappa == 'scan', optionally set bounds to scan kappa values between, in either a linear or log-scale.\n",
    "scan_scale = 'log' # or linear\n",
    "min_kappa = None\n",
    "max_kappa = None\n",
    "out_script = 'train_out.sh' # script file to save kappa-scanning learn_model() commands \n",
    "\n",
    "# total number of models to spool\n",
    "n_models = 15\n",
    "\n",
    "# Select platform to run models on\n",
    "cluster_type = 'local' # currently supported cluster_types = 'local' or 'slurm'\n",
    "run_cmd = False # if True, runs the commands via os.system(...), script must be run manually otherwise\n",
    "\n",
    "## SLURM PARAMETERS\n",
    "## only edit these parameters if cluster_type == 'slurm'\n",
    "memory = '16GB'\n",
    "wall_time='3:00:00'\n",
    "partition='short'\n",
    "\n",
    "progress_paths = update_progress(progress_filepath, 'model_path', model_path)\n",
    "progress_paths = update_progress(progress_filepath, 'model_session_path', session_path)\n",
    "\n",
    "learn_model_command(progress_paths, hold_out=hold_out, nfolds=nfolds, num_iter=num_iter, max_states=max_states,\n",
    "                    npcs=npcs, kappa=kappa, separate_trans=separate_trans, robust=robust,\n",
    "                    checkpoint_freq=checkpoint_freq, use_checkpoint=use_checkpoint, select_groups=select_groups,\n",
    "                    cluster_type=cluster_type, min_kappa=min_kappa, scan_scale=scan_scale,\n",
    "                    max_kappa=max_kappa, n_models=n_models, run_cmd=run_cmd, output_dir=modeling_session_path,\n",
    "                    out_script=out_script, memory=memory, wall_time=wall_time, partition=partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once training is complete, your model will be saved in your base directory (shown below). \n",
    "```\n",
    ".\n",
    "├── _pca/ \n",
    "├── aggregate_results/ \n",
    "├── config.yaml\n",
    "├── modeling_session/ ***\n",
    "├   └── model.p ***\n",
    "├── moseq2-index.yaml/\n",
    "├── session_1/\n",
    "└── session_2/\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Best Model Fit\n",
    "\n",
    "Use this feature to determine whether the trained model has captured median syllable durations that match the principal components changepoints.\n",
    "\n",
    "This feature can also return the best model from a list of models found in the `progress_paths['model_session_path']`.\n",
    "\n",
    "Below are examples of some comparative distributions that you can expect when using this tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from moseq2_viz.gui import get_best_fit_model\n",
    "from moseq2_app.gui.progress import update_progress\n",
    "\n",
    "output_file = join(progress_paths['plot_path'], 'model_vs_pc_changepoints')\n",
    "\n",
    "best_model_fit = get_best_fit_model(progress_paths, plot_all=True)\n",
    "progress_paths = update_progress(progress_filepath, 'model_path', best_model_fit['best model - duration'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Notebook End\n",
    "\n",
    "## Go to the [Interactive-Model-Results.ipynb](./Interactive-Model-Results-Exploration.ipynb) Jupyter Notebook to analyze model results."
   ]
  }
 ],
 "metadata": {
  "finalized": {
   "timestamp": 1623072349278,
   "trusted": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "303.984px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
