{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moseq2 App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1PxTnCMsrk3hRHPnEjqGDzq1oPkTYfzj0\">\n",
    "\n",
    "MoSeq2 is a software toolkit for unsupervised modeling and characterization of animal behavior. Moseq takes depth recordings of animals as input and outputs a rich description of behavior as a series of reused and stereotyped motifs called 'syllables'. \n",
    "\n",
    "This notebook begins with depth recordings (see the [data acquisition overview](#Data-Acquisition-Overview) below) and transforms this data through the steps of: \n",
    "\n",
    "- **[Extraction](#Raw-Data-Extraction)**: The animal is segmented from the background and its position and heading direction are aligned across frames.\n",
    "- **[Dimensionality reduction](#Principal-Component-Analysis-(PCA))**: Raw video is de-noised and transformed to low-dimensional pose trajectories using principal component analysis (PCA).\n",
    "- **[Model training](#ARHMM-Modeling)**: Pose trajectories are modeled using an autoregressive hidden Markov model (AR-HMM), producing a sequence of syllable labels.\n",
    "\n",
    "__The model output can be analyzed using the [Interactive Results Exploration](./Interactive-Model-Results-Exploration.ipynb) notebook.__\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Wiki](https://github.com/dattalab/moseq2-app/wiki) with instructions on data acquisition and command line options\n",
    "- PDF documentation of all MoSeq functions: [Extract](https://github.com/dattalab/moseq2-extract/blob/release/Documentation.pdf), [PCA](https://github.com/dattalab/moseq2-pca/blob/release/Documentation.pdf), [Model](https://github.com/dattalab/moseq2-model/blob/release/Documentation.pdf)\n",
    "- Publications:\n",
    "    - [Mapping Sub-Second Structure in Mouse Behavior](http://datta.hms.harvard.edu/wp-content/uploads/2018/01/pub_23.pdf)\n",
    "    - [The Striatum Organizes 3D Behavior via Moment-to-Moment Action Selection](http://datta.hms.harvard.edu/wp-content/uploads/2019/06/Markowitz.final_.pdf)\n",
    "    - [Revealing the structure of pharmacobehavioral space through motion sequencing](https://www.nature.com/articles/s41593-020-00706-3)\n",
    "    - [Q&A: Understanding the composition of behavior](http://datta.hms.harvard.edu/wp-content/uploads/2019/06/Datta-QA.pdf)\n",
    "    \n",
    "### Feedback\n",
    "\n",
    "For general feedback and feature requests, please fill out [this survey](https://forms.gle/FbtEN8E382y8jF3p6).\n",
    "    \n",
    "### Data Acquisition Overview\n",
    "MoSeq2 takes animal depth recordings as input. We we have developed a [data acquisition pipeline](https://github.com/dattalab/moseq2-app/wiki/Setup:-acquisition-software) for the second generation `Xbox Kinect` depth camera. We suggest following our [data acquisition tutorial](https://github.com/dattalab/moseq2-app/wiki/Acquisition) for doing recordings. MoSeq2 also accepts depth recordings from the `Azure Kinect` camera as well as the `Intel RealSense` using their standard data acquisitions pipelines.\n",
    "\n",
    "**We recommend recording more than 10 hours of depth video (~1 million frames at 30 frames per second) to ensure quality MoSeq models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1h2GYECyEuTMlM7Rx3Q3lMVBdWEm1F0S5\">\n",
    "\n",
    "### Check to see if you're running python from the correct conda enviornment\n",
    "\n",
    "If you performed the recommended installation, you should see the `sys.executable` path point to the python path within the `moseq2-app` environment, i.e.,\n",
    "```python\n",
    "import sys\n",
    "print(sys.executable)\n",
    "# /Users/username/miniconda3/envs/moseq2-app/bin/python\n",
    "```\n",
    "\n",
    "### Check if the dependencies are found\n",
    "\n",
    "Run the following cell to check if `moseq2-app` is installed in your current conda kernel. The latest working version number is `0.2.1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import moseq2_app\n",
    "\n",
    "print('Python path:', sys.executable)\n",
    "print('MoSeq2 app version:', moseq2_app.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data file organization\n",
    "\n",
    "The currently accepted depth data extensions are:\n",
    "- `.dat` (raw depth files from our kinect2 data acquisition software)\n",
    "- `.tar.gz` (compressed depth files from our kinect2 data acquisition software)\n",
    "- `.avi` (compressed depth files from the `moseq2-extract` command line interface)\n",
    "- `.mkv` (generated from Microsoft's recording software for the Azure Kinect)\n",
    "\n",
    "To run this notebook, create a master folder with a copy of this notebook, and a separate subfolders for each recording file (see example directory structure below). \n",
    "\n",
    "```\n",
    ".\n",
    "└── Data_Directory/\n",
    "    ├── Main-MoSeq2-Notebook.ipynb (running)\n",
    "    ├── session_1/ ** - the folder containing all of a single session's data\n",
    "    ├   ├── depth.dat        # depth data - the recording itself\n",
    "    ├   ├── depth_ts.txt     # timestamps - csv/txt file of the frame timestamps\n",
    "    ├   └── metadata.json    # metadata - json file that contains the rodent's info (group, subjectName, etc.)\n",
    "    ...\n",
    "    ├── session_2/ **\n",
    "    ├   ├── depth.dat\n",
    "    ├   ├── depth_ts.txt\n",
    "    └── └── metadata.json\n",
    "\n",
    "```\n",
    "\n",
    "__Note: if your data was acquired using an Azure Kinect or Intel RealSense depth camera, you will not have `depth_ts.txt` or `metadata.json` in your session directories. Before extraction you need to manually create a `metadata.json` file if you wish to identify sessions based on the session name or mouse ID.__ The metadata.json folder should minimally contain the following:\n",
    "\n",
    "```json\n",
    "{\"SessionName\": \"example session\", \"SubjectName\": \"example subject\", \"StartTime\": \"optional\"}\n",
    "```\n",
    "\n",
    "### Notebook Progress File\n",
    "\n",
    "This notebook generates a `progress.yaml` file that stores the filepaths to data generated from this notebook, including:\n",
    "- extraction data files\n",
    "- PC scores of the extractions\n",
    "- model results\n",
    "\n",
    "If your notebook kernel is shutdown, you can load the progress file to 'restore' your progress. The progress file does **not** track MoSeq pipeline operations that were executed outside of this notebook (for example, if you were to run PCA using the command line interface). If necessary, you can manually modify the paths in the progress file or the corresponding `progress_paths` dictionary to access the output of these external operations.\n",
    "\n",
    "__To restore previously computed variables, look for the cells following the `Restore Progress Variables` label.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore Progress Variables\n",
    "\n",
    "- Use this cell to load your notebook analysis progress. We recommend running this notebook from the folder where your data is located so the generated media will display properly. In that case, you can specify the `base_dir` as `./` (or the current folder).\n",
    "- The `base_dir` will be stored in the progress dict and will be reused throughout the notebooks.\n",
    "\n",
    "The `check_progress` function will print progress bars for each pipeline step in the notebook. \n",
    "- The extraction progress bar indicates total the number of extracted sessions detected in the provided `base_dir` path.\n",
    "- It prints the session names that haven't been extracted. __Note: the progress does not reflect the contents of the aggregate_results/ folder.__\n",
    "- The remainder of the progress bars are derived from reading the paths in the `progress_paths` dictionary, filling up the bar if the included paths are found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from moseq2_app.gui.progress import check_progress, restore_progress_vars\n",
    "\n",
    "# Add the path to your data folder here.\n",
    "# We recommend that you run this notebook in the same folder as your data. In that case, you don't have to change base_dir\n",
    "base_dir = './'\n",
    "progress_filepath = join(base_dir, 'progress.yaml')\n",
    "\n",
    "progress_paths = restore_progress_vars(progress_filepath, init=True, overwrite=False)\n",
    "check_progress(progress_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Configuration Files\n",
    "\n",
    "The `config.yaml` will be used to hold all configurable parameters for all steps in the MoSeq pipeline. Parameters will be added to this file as you progress through the notebook. The config file can be used to run an identical pipeline in future analyses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from moseq2_app.gui.progress import update_progress\n",
    "from moseq2_extract.gui import generate_config_command\n",
    "\n",
    "config_filepath = join(progress_paths['base_dir'], 'config.yaml')\n",
    "\n",
    "print(f'generating file in path: {config_filepath}')\n",
    "generate_config_command(config_filepath)\n",
    "progress_paths = update_progress(progress_filepath, 'config_file', config_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A configuration file has been created in the base directory (depicted below).\n",
    "\n",
    "```\n",
    ".\n",
    "└── Data_Directory/\n",
    "    ├── config.yaml **\n",
    "    ├── session_1/ \n",
    "    ├   ├── depth.dat        \n",
    "    ├   ├── depth_ts.txt     \n",
    "    ├   └── metadata.json    \n",
    "    ...\n",
    "    ├── session_2/ \n",
    "    ├   ├── depth.dat\n",
    "    ├   ├── depth_ts.txt\n",
    "    └── └── metadata.json\n",
    "```\n",
    "\n",
    "### Download a Flip File\n",
    "\n",
    "MoSeq2 uses a Random Forest flip classifier to guarantee that the mouse is always pointed to the right after cropping and rotationally aligning the depth videos. The flip classifiers we provide __are trained for experiments run with C57BL/6 mice using with Kinect v2 depth cameras__.\n",
    "\n",
    "If your dataset does not work with our pre-trained flip classifiers, we provide a [flip-classifier training notebook](https://github.com/dattalab/moseq2-app/tree/jupyter/). After using this notebook, add the path of your custom classifier to the `config.yaml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_extract.gui import download_flip_command\n",
    "# selection=0 - large mice with fibers (default)\n",
    "# selection=1 - adult male C57s\n",
    "# selection=2 - mice with Inscopix cables\n",
    "download_flip_command(progress_paths['base_dir'], config_filepath, selection=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Data Extraction\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1XtDo6sVtvG0Grp5pDgLbFcli2_hRcTZK\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive ROI Detection Tool (optional)\n",
    "\n",
    "Use this interactive tool to optimized the extraction parameters prior to extracting all of your data. Most of the parameters are related to detecting the region the mouse occupies (the ROI). This tool can also be used to catch possibly corrupted or inconsistent sessions, and to diagnose ROI detection/extraction errors.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td style=\"width: 45%;\">\n",
    "            <ol>\n",
    "                <li style=\"text-align:left; font-size:14px\">\n",
    "                    Execute the code below to launch the ROI Detection Tool.\n",
    "                </li>\n",
    "                <li style=\"text-align:left; font-size:14px\">\n",
    "                    Click on any row in the session selector to load that session's data to the view.\n",
    "                </li>\n",
    "                <li style=\"text-align:left; font-size:14px\">\n",
    "                    If the indicator next to the session's name is green, then the session is considered ready for extraction. A red indicator can either mean the session has not been checked yet, or its extraction parameter set is incorrect.\n",
    "                </li>\n",
    "                <li style=\"text-align:left; font-size:14px\">\n",
    "                    Adjust the Depth Range Selector to include the depth range of the detected bucket floor distance (which can be found by hovering over the Background image with your mouse). You can also manually enter slider values by clicking on the numbers.\n",
    "                </li>\n",
    "                <li style=\"text-align:left; font-size:14px\">\n",
    "                    If the mouse seems to be cropped when at the bucket edge, increase the \"dilate iterations\" settings to enlarge the size of the included floor area.\n",
    "                </li>\n",
    "                <li style=\"text-align:left; font-size:14px\">\n",
    "                    Use the Rodent Height Threshold Slider to remove any noise/speckle from the bucket floor or walls. \n",
    "                    <ul>\n",
    "                        <li style=\"text-align:left; font-size:14px\">\n",
    "                            Ensure the min height parameter is small enough to only filter out floor reflections.\n",
    "                        </li>\n",
    "                        <li style=\"text-align:left; font-size:14px\">\n",
    "                            Ensure the max height parameter is large enough to include the largest possible mouse height, (i.e., when the mouse is rearing). A reasonable value is around 100 mm for Kinect v2 recordings.\n",
    "                        </li>\n",
    "                        <li style=\"text-align:left; font-size:14px\">\n",
    "                            Hover over the mouse in either of the bottom two plots to explore its height.\n",
    "                        </li>\n",
    "                    </ul>\n",
    "                </li>\n",
    "                <li style=\"text-align:left; font-size:14px\">\n",
    "                    Use the \"current frame\" slider to change the displayed session frame in the bottom 2 plots.\n",
    "                </li>\n",
    "                <li style=\"text-align:left; font-size:14px\">\n",
    "                    Change the frame range slider values to adjust the segments of the video to extract, then click the \"Extract Sample\" button to trigger an extraction and view the results.\n",
    "                </li>\n",
    "                <li style=\"text-align:left; font-size:14px\">\n",
    "                    Once you have found a satisfactory set of parameters, click \"Check All Sessions\" to test the parameters on all sessions. A set of filters described <a href=\"https://github.com/dattalab/moseq2-app/wiki/Analysis:-extraction#check-all-session-protocol\">here</a> will be applied to detect possible poor extractions.\n",
    "                </li>\n",
    "                <li style=\"text-align:left; font-size:14px\">\n",
    "                    If no sessions are flagged, click \"Save Parameters\". The parameters for each session will be written in the overall project config file and in each session-specific config file. \n",
    "                </li>\n",
    "                <li style=\"text-align:left; font-size:14px\">\n",
    "                    If a session is flagged, click on it in the Session Selector and a text indicator will appear with error details. Readjust the parameters until the session passes and then save the parameters. The \"Mark Passing\" button can be used to manually accept a session's parameter set.\n",
    "                </li>\n",
    "            </ol>\n",
    "            <p style=\"text-align:left; font-size:14px\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"https://drive.google.com/uc?export=view&id=1iIj92Wl0Uezn_ehjvGnwV2YzGp8Pir6f\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "__Note: if cell seems to be running out of memory after first use, set `compute_all_bgs=False` to reduce the memory pressure.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import ruamel.yaml as yaml\n",
    "from moseq2_app.gui.progress import update_progress\n",
    "from moseq2_app.main import interactive_roi_detector\n",
    "\n",
    "session_config_path = join(progress_paths['base_dir'], 'session_config.yaml')\n",
    "progress_paths = update_progress(progress_filepath, 'session_config', session_config_path)\n",
    "\n",
    "with open(progress_paths['config_file'], 'r') as f:\n",
    "    config_data = yaml.safe_load(f)\n",
    "\n",
    "config_data['camera_type'] = 'auto' # 'kinect', 'azure' or 'realsense'\n",
    "config_data['crop_size'] = (80, 80)\n",
    "config_data['output_dir'] = 'proc' # the subfolder extracted data is saved to\n",
    "\n",
    "# if using azure or realsense, increase the noise_tolerance for ROI detection\n",
    "config_data['noise_tolerance'] = 30\n",
    "\n",
    "# OPTIONAL additional parameters\n",
    "# config_data['flip_classifier'] = './alternative-flip-classifier.pkl' # updated flip classifier path\n",
    "# config_data['gaussfilter_space'] = [2.5, 2] # spatial filtering kernel size\n",
    "# config_data['medfilter_time'] = (3,) # temporal filtering kernel size\n",
    "\n",
    "# Filtering out head-fixed cables?\n",
    "# config_data['cable_filter_iters'] = 3 # number of cable filtering iterations\n",
    "# config_data['cable_filter_size'] = (7, 7) # cable spatial filter kernel size\n",
    "\n",
    "with open(progress_paths['config_file'], 'w') as f:\n",
    "    yaml.safe_dump(config_data, f)\n",
    "\n",
    "compute_all_bgs = True # If False, only computes the first background on launch\n",
    "                \n",
    "autodetect_depths = False # If True, will readjust the bg_depth_range for each session \n",
    "\n",
    "interactive_roi_detector(progress_paths, compute_all_bgs=compute_all_bgs, autodetect_depths=autodetect_depths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore Progress Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_app.gui.progress import restore_progress_vars\n",
    "\n",
    "progress_filepath = './progress.yaml'\n",
    "\n",
    "progress_paths = restore_progress_vars(progress_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Session(s)\n",
    "\n",
    "__Note: If sessions are not listed when running the cell, ensure your selected extension matches that of your depth files.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from moseq2_extract.gui import extract_found_sessions\n",
    "\n",
    "# include the file extensions for the depth files you would like to search for and extract.\n",
    "extensions = ['.avi', '.dat'] # .avi, .dat, and/or .mkv\n",
    "\n",
    "# for the option to extract individual sessions, set extract_all=False\n",
    "# to overwrite previously extracted sessions, set skip_extracted=False\n",
    "extract_found_sessions(progress_paths['base_dir'], progress_paths['config_file'], extensions, extract_all=True, skip_extracted=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what your directory structure should look like once the process is complete:\n",
    "\n",
    "```\n",
    ".\n",
    "├── config.yaml\n",
    "├── session_1/\n",
    "├   ...\n",
    "├   └── proc/ **\n",
    "├   ├   ├── roi.tiff\n",
    "├   ├   ...\n",
    "├   ├   ├── results_00.yaml ** (.yaml file storing extraction parameters)\n",
    "├   ├   ├── results_00.h5 ** (.h5 file storing extraction)\n",
    "├   └   └── results_00.mp4 ** (extracted video)\n",
    "└── session_2/\n",
    "├   ...\n",
    "├   └── proc/ **\n",
    "├   ├   ├── roi.tiff\n",
    "├   ├   ...\n",
    "├   ├   ├── results_00.yaml **\n",
    "├   ├   ├── results_00.h5 **\n",
    "└   └   └── results_00.mp4 **\n",
    "        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Extraction Validation Tests (optional)\n",
    "\n",
    "Once all the extractions are complete, use the following cell to run data validation tests. The tests can output either an error or a warning. \n",
    "- An __error__ indicates that the session is corrupted in some way and should be excluded from PCA and Modeling.\n",
    "- A __warning__ indicates that one or more sessions are statistical outliers.\n",
    "  - A warning can indicate that the session may need to be inspected prior to continuing into the PCA step. \n",
    "  - Warnings can be ignored when they are consistent with experimental design (e.g. abnormally high velocity in an animal that recieved a stimulant drug). \n",
    "\n",
    "__Error tests__: \n",
    "- Count Dropped Frames: an error is raised if a session is missing >5% of the frames based on timestamps (requires a timestamp file).\n",
    "- Missing Mouse Check: raises an error if a mouse is missing from the video for any reason for >5% of the session's total frames.\n",
    "- Scalar Anomaly: raises an error if >5% of a session's computed scalar values are NaN.\n",
    "\n",
    "__Warning tests__:\n",
    "- Size Anomaly: Warning is raised when a mouse's captured body size is less than 2 standard deviations from the mean size throughout the session.\n",
    "- Scalar Anomaly: Warning is raised for a session if the trained [EllipticEnvelope](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.EllipticEnvelope.html) model classifies it as an outlier. \n",
    "- Position Anomaly: There are two cases that raise warnings:\n",
    "    1. Mouse is stationary for >5% of the session.\n",
    "    2. Mouse's position distribution is at least 2 standard deviations away from the mean of all the sessions, measured using Kullback–Leibler divergence.\n",
    "     - This anomaly can indicate that a mouse has explored a much larger or smaller region of the arena compared to the other recordings in the datset.\n",
    "\n",
    "To diagnose certain scalar anomalies, use the [Scalar Summary Cell](#Compute-Scalar-Summary) below to graph any desired scalar value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_app.main import validate_extractions\n",
    "\n",
    "validate_extractions(progress_paths['base_dir']) # path to pre-existing aggregate_results/ folder is also permissible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Extraction Output (optional)\n",
    "\n",
    "Run the following cell to view the extraction output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_app.main import preview_extractions\n",
    "\n",
    "preview_extractions(progress_paths['base_dir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate your results into one folder and generate an index file.\n",
    "\n",
    "The following cell will search for the `proc/` subfolders containing the extraction output, and copy them to a single `aggregate_results/` folder. An index file called `moseq2-index.yaml` will also be generated with metadata for all extracted sessions. The index file can be used to group recordings by experimental condition for downstream analysis. \n",
    "Initially, each session as assigned to a single group called \"default\". We provide an interface for re-assigning group labels below.\n",
    "\n",
    "The `aggregate_results/` folder contains all the data you need to run the rest of the pipeline. The PCA and modeling step will use data in this folder.\n",
    "\n",
    "__Important Note: The index file contains UUIDs to map each session to a specific extraction. These UUIDs are referenced throughout the pipeline, so if you re-extract a session and re-aggregate your data, ensure all the UUIDs in the index file are up-to-date BEFORE running the PCA step.__ Not updating the index file will likely cause `KeyError`s to occur in the PCA and modeling steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from moseq2_app.gui.progress import update_progress\n",
    "from moseq2_extract.gui import aggregate_extract_results_command\n",
    "\n",
    "recording_format = '{start_time}_{session_name}_{subject_name}' # filename formats for the copied extracted data files\n",
    "\n",
    "# directory NAME to save all metadata+extracted videos to with above respective name format\n",
    "aggregate_results_dirname = 'aggregate_results/'\n",
    "\n",
    "train_data_dir = join(progress_paths['base_dir'], aggregate_results_dirname)\n",
    "update_progress(progress_filepath, 'train_data_dir', train_data_dir)\n",
    "\n",
    "# the subpath indicates to only aggregate extracted session paths with that subpath, only change if aggregating data from a different location\n",
    "index_filepath = aggregate_extract_results_command(progress_paths['base_dir'], recording_format, aggregate_results_dirname)\n",
    "progress_paths = update_progress(progress_filepath, 'index_file', index_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aggregate results folder will be saved in your base directory, e.g.\n",
    "\n",
    "```\n",
    ".\n",
    "├── aggregate_results/ **\n",
    "├   ├── session_1_results_00.h5 ** # session 1 compressed extraction + metadata \n",
    "├   ├── session_1_results_00.yaml **\n",
    "├   ├── session_1_results_00.mp4 ** # session 1 extracted video\n",
    "├   ├── session_2_results_00.h5 ** # session 2 compressed extraction + metadata \n",
    "├   ├── session_2_results_00.yaml **\n",
    "├   └── session_2_results_00.mp4 ** # session 2 extracted video\n",
    "├── config.yaml\n",
    "├── moseq2-index.yaml ** # index file\n",
    "├── session_1/\n",
    "└── session_2/\n",
    "```\n",
    "\n",
    "## Specify Groups\n",
    "\n",
    "Sessions can be given \"group\" labels in the moseq2-index.yaml for analyses comparing different cohorts or experimental conditions. This step requires that all your sessions have a metadata.json file containing a session name. Run the cell below to launch the group assignment GUI\n",
    "\n",
    "- Click on the column names to sort the index file.\n",
    "- Enter your desired group name in the text input and click `Set Group` to update all the associated session rows.\n",
    "- Once all your groups are set, click the `Update Index File` button to save current group assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_app.main import interactive_group_setting\n",
    "\n",
    "interactive_group_setting(progress_paths['index_file'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Scalar Summary (optional)\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1wAHj1d5u0GeSa2_iJi7agjbqllbwXFKz\">\n",
    "\n",
    "Use the following command to plot a summary of scalar values for each group, such as average velocity, height, etc.\n",
    "- Hold [CTRL]/[Command] and click on the Selector rows to select multiple scalars to plot.\n",
    "- Hover over any of the data points to display the session information.\n",
    "- Click on the legend items to show/hide groups from the plot. Double click an item to only show a single group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_app.main import interactive_scalar_summary\n",
    "\n",
    "interactive_scalar_summary(progress_paths['index_file'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Position Heatmaps For Each Session (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from moseq2_viz.gui import plot_verbose_position_heatmaps\n",
    "\n",
    "output_file = join(progress_paths['plot_path'], 'session_heatmaps') \n",
    "verbose_heatmap_fig = plot_verbose_position_heatmaps(progress_paths['index_file'], output_file)\n",
    "verbose_heatmap_fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Group Mean Position Summary (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from moseq2_viz.gui import plot_mean_group_position_heatmaps_command\n",
    "\n",
    "output_file = join(progress_paths['plot_path'], 'group_heatmaps') \n",
    "group_mean_heatmap_fig = plot_mean_group_position_heatmaps_command(progress_paths['index_file'], output_file)\n",
    "group_mean_heatmap_fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1KdNmEf_BcME5u39-mt75ROCDLDPzO7k-\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore Progress Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_app.gui.progress import restore_progress_vars\n",
    "\n",
    "progress_filepath = './progress.yaml'\n",
    "\n",
    "progress_paths = restore_progress_vars(progress_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting PCA\n",
    "\n",
    "Fit PCA to your extracted data to determine the principal components (PCs) that explain the largest possible variance in your dataset. The PCs should look smooth and well defined like the examples below. The PCs should explain >90% of the variance in the dataset using around 10 PCs. If this isn't the case, consult the [table of possible pathologies ](#Possible-PCA-Pathologies). If running PCA locally, progress can be monitored using the [dask server](https://localhost:8787/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import ruamel.yaml as yaml\n",
    "from moseq2_pca.gui import train_pca_command\n",
    "from moseq2_app.gui.progress import update_progress\n",
    "\n",
    "pca_filename = 'pca' # Name of your PCA model h5 file to be saved\n",
    "pca_dirname = join(progress_paths['base_dir'], '_pca/') # Directory to save your computed PCA results\n",
    "\n",
    "with open(progress_paths['config_file'], 'r') as f:\n",
    "    config_data = yaml.safe_load(f)\n",
    "\n",
    "# PCA parameters you may need to configure\n",
    "config_data['overwrite_pca'] = False\n",
    "config_data['gaussfilter_space'] = (1.5, 1) # Spatial filter for data (Gaussian)\n",
    "config_data['medfilter_space'] = [0] # Median spatial filter\n",
    "config_data['medfilter_time'] = [0] # Median temporal filter\n",
    "\n",
    "# If dataset includes head-attached cables, set missing_data=True\n",
    "config_data['missing_data'] = False # Set True for dataset with missing/dropped frames to reconstruct respective PCs.\n",
    "config_data['missing_data_iters'] = 10 # Number of times to iterate over missing data during PCA\n",
    "config_data['recon_pcs'] = 10 # Number of PCs to use for missing data reconstruction\n",
    "\n",
    "# Dask Configuration\n",
    "config_data['dask_port'] = '8787' # port to access Dask Dashboard\n",
    "\n",
    "# UNCOMMENT to use SLURM\n",
    "# config_data['cluster_type'] = 'slurm'\n",
    "# config_data['nworkers'] = 8 # number of spawned jobs\n",
    "# config_data['queue'] = 'short' # partition\n",
    "# config_data['memory'] = '40GB' # amount of memory per worker\n",
    "# config_data['cores'] = 1 # number of cores per worker\n",
    "# config_data['wall_time'] = '01:00:00' # worker time limit\n",
    "\n",
    "# UNCOMMENT if recordings contain occlusions (e.g. from overhead cables)\n",
    "# config_data['missing_data'] = True\n",
    "\n",
    "with open(progress_paths['config_file'], 'w') as f:\n",
    "    yaml.safe_dump(config_data, f)\n",
    "\n",
    "progress_paths = update_progress(progress_filepath, 'pca_dirname', pca_dirname)\n",
    "\n",
    "# will train on data in aggregate_results/\n",
    "train_pca_command(progress_paths, pca_dirname, pca_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once complete, a new directory `_pca` will be created containing the following data:\n",
    "```\n",
    ".\n",
    "├── _pca/ **\n",
    "├   ├── pca.h5 ** # pca model compressed file\n",
    "├   ├── pca.yaml  ** # pca model YAML metadata file\n",
    "├   ├── pca_components.png **\n",
    "├   └── pca_scree.png **\n",
    "├── aggregate_results/\n",
    "├── config.yaml\n",
    "├── moseq2-index.yaml\n",
    "├── session_1/\n",
    "└── session_2/\n",
    "\n",
    "```\n",
    "\n",
    "View your `computed PCs` and `scree plot` in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from IPython.display import display, Image\n",
    "images = [join(progress_paths['pca_dirname'], 'pca_components.png'), \n",
    "          join(progress_paths['pca_dirname'], 'pca_scree.png')]\n",
    "for im in images:\n",
    "    display(Image(im))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible PCA Pathologies\n",
    "\n",
    "<table style=\"width: 100%;\">\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th></th>\n",
    "      <th>Good PCA Output Examples</th>\n",
    "      <th style=\"text-align:center;\">Bad Scree Plot Example</th>\n",
    "      <th style=\"text-align:center;\">Bad Principal Components Example</th>\n",
    "    </tr>  \n",
    "    <tr>\n",
    "      <th style=\"text-align:center;\">Pathology Description</th>\n",
    "      <th style=\"text-align:center;\"></th>\n",
    "      <td style=\"text-align:center;\">Cannot achieve a explained variance of over 90% from less than 15 Principal Components (PCs).</td>\n",
    "      <td style=\"text-align:center;\">PCs look noisy, or are not representative of realistic mouse body regions.</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th style=\"text-align:center;\">Reference Examples</th>\n",
    "      <th style=\"text-align:center;\">\n",
    "        <ul>\n",
    "            <li>Components<br>\n",
    "                <img src=\"https://drive.google.com/uc?export=view&id=1dX5Gpd3PKL4vfVviLeP0CqBrz9PW37Au\" width=350 height=350></li><br><br>\n",
    "            <li>Scree Plot<br>\n",
    "                <img src=\"https://drive.google.com/uc?export=view&id=12uqsBYuWCjpUQ6QrAjo35MnwYDzHqnge\" width=350 height=350>\n",
    "            <br>\"90.65% in 7 PCs\"</li>\n",
    "        </ul>\n",
    "      </th>\n",
    "      <td><img src=\"https://drive.google.com/uc?export=view&id=1DazNIPlGLAIPPQNeGF3eLR2l1QBnek4N\" width=350 height=350></td>\n",
    "      <td><img src=\"https://drive.google.com/uc?export=view&id=1xKHn0kEcs26R78aRRZwtPV3EOZ7h-qa9\" width=350 height=350></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th style=\"text-align:center;\">Image Analysis Solutions</th>\n",
    "      <th style=\"text-align:center;\"></th>\n",
    "      <td>\n",
    "        <ul>\n",
    "          <li style=\"text-align:left;\">Check if the crop size is too large, if so, decrease it and re-extract your data.</li>\n",
    "          <li style=\"text-align:left;\">Try (incrementally) adjusting the spatial and temporal filtering kernel sizes in the PCA step. Generally, increasing temporal smoothing will aid in increasing explained variance, but can potentially throw out data.</li>\n",
    "        </ul>\n",
    "      </td>\n",
    "      <td>\n",
    "          <ul>\n",
    "              <li style=\"text-align:left;\">Ensure that an appropriate amount of spatial and temporal filtering is applied.</li>\n",
    "              <li style=\"text-align:left;\">If you set missing_data=True, adjust spatial and temporal filtering, and try adjusting the amount of PCs used for reconstruction (the recon_pcs parameter).</li>\n",
    "          </ul>\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th style=\"text-align:center;\">General Solutions</th>\n",
    "      <th style=\"text-align:center;\"></th>\n",
    "      <td style=\"text-align:center;\">\n",
    "          <ul>\n",
    "          <li style=\"text-align:left;\">If there are cable occlusions, try setting missing_data=True. Using an iterative PCA to reconstruct the PCs can aid in increasing the explained variance ratio.</li>\n",
    "          <li style=\"text-align:left;\">Increase the size of your dataset. If your dataset is too small, it may contribute to overfit PCs.</li>\n",
    "        </ul>\n",
    "      </td> <!-- G -->\n",
    "      <td style=\"text-align:center;\">Acquire and extract more data, then try again.</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Principal Component Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from moseq2_pca.gui import apply_pca_command\n",
    "from moseq2_app.gui.progress import update_progress\n",
    "\n",
    "scores_filename = 'pca_scores' # name of the scores file to compute and save\n",
    "\n",
    "scores_file = join(progress_paths['pca_dirname'], scores_filename + '.h5') # path to input PC scores file to model\n",
    "progress_paths = update_progress(progress_filepath, 'scores_path', scores_file)\n",
    "\n",
    "apply_pca_command(progress_paths, scores_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output if this step is saved in a file called ```pca_scores.h5``` in the  ```_pca directory```. \n",
    "```\n",
    ".\n",
    "├── _pca/\n",
    "├   ├── pca.h5\n",
    "├   ├── pca.yaml\n",
    "├   ├── pca_scores.h5  ** # scores file\n",
    "├   ├── pca_components.png\n",
    "├   └── pca_scree.png\n",
    "├── aggregate_results/\n",
    "├── config.yaml\n",
    "├── moseq2-index.yaml\n",
    "├── session_1/\n",
    "└── session_2/\n",
    "\n",
    "```\n",
    "\n",
    "### Computing Model-Free Changepoints (Optional) \n",
    "\n",
    "This step can be used to determine a target syllable duration for the modeling step. Typically the distribution of change-point durations is smooth, left-skewed and centered around 0.3 seconds. If that is not the case, consult the [table of possible pathologies](#Possible-Model-Free-Changepoints-Pathologies) below.\n",
    "\n",
    "__Note: the parameters below are configured for C57 mouse data, and have not been tested for other strains/species.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ruamel.yaml as yaml\n",
    "from moseq2_app.gui.progress import update_progress\n",
    "from moseq2_pca.gui import compute_changepoints_command\n",
    "\n",
    "with open(progress_paths['config_file'], 'r') as f:\n",
    "    config_data = yaml.safe_load(f)\n",
    "\n",
    "changepoints_filename = 'changepoints' # name of the changepoints images to generate\n",
    "\n",
    "# Changepoint computation parameters you may want to configure\n",
    "config_data['threshold'] = 0.5 # Peak threshold to use for changepoints\n",
    "config_data['dims'] = 300 # Number of random projections to compare the computed principal components with\n",
    "\n",
    "with open(progress_paths['config_file'], 'w') as f:\n",
    "    yaml.safe_dump(config_data, f)\n",
    "\n",
    "progress_paths = update_progress(progress_filepath, 'changepoints_path', changepoints_filename)\n",
    "compute_changepoints_command(progress_paths['train_data_dir'], progress_paths, changepoints_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The changepoints plot will be generated and saved in the ```_pca``` directory.\n",
    "```\n",
    ".\n",
    "├── _pca/ \n",
    "├   ├── pca.h5\n",
    "├   ├── pca_scores.h5\n",
    "├   ...\n",
    "├   └── changepoints_dist.png **\n",
    "├── aggregate_results/ \n",
    "├── config.yaml\n",
    "├── moseq2-index.yaml\n",
    "├── session_1/\n",
    "└── session_2/\n",
    "```\n",
    "\n",
    "View ```changepoints_dist.png``` using the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from IPython.display import display, Image\n",
    "\n",
    "changepoints_filename = 'changepoints'\n",
    "display(Image(join(progress_paths['pca_dirname'], changepoints_filename + '_dist.png')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible Model-Free Changepoints Pathologies\n",
    "\n",
    "<table style=\"width: 100%;\">\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th></th>\n",
    "      <th style=\"text-align:center;\">Good Changepoint Analysis Example</th>\n",
    "      <th style=\"text-align:center;\">Poor Changepoints Analysis Example</th>\n",
    "    </tr>  \n",
    "    <tr>\n",
    "      <th style=\"text-align:center;\">Pathology Description</th>\n",
    "      <td style=\"text-align:center;\"></td>\n",
    "      <td style=\"text-align:center;\">Model-free syllable changepoint distances distribution is incorrectly skewed/too sparse and/or changepoint mode duration is less than 0.2s</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th style=\"text-align:center;\">Reference Example</th>\n",
    "      <td><img src=\"https://drive.google.com/uc?export=view&id=1zlIaunlhwu0dX-Fw8jk3Xqp9K4FxJLTn\" width=350 height=350></td>\n",
    "      <td><img src=\"https://drive.google.com/uc?export=view&id=1RhdSyvvy9TgoCv0srfuQWPStqe-N1_3C\" width=350 height=350></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th style=\"text-align:center;\">General Solutions</th>\n",
    "      <td style=\"text-align:center;\"></td>\n",
    "      <td>\n",
    "          <ul>\n",
    "              <li style=\"text-align:left;\">Try retraining the PCA with adjusted spatial and temporal filtering kernel sizes.</li>\n",
    "              <li style=\"text-align:left;\">Ensure your extracted data is correct with minimal flips. If the extraction version of the mouse is too noisy, then the PC trajectories cannot be accurately applied to the data.</li>\n",
    "              <li style=\"text-align:left;\">Get more data and try again.</li>\n",
    "          </ul>\n",
    "      </td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARHMM Modeling\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1pAiffIWGsLtbu6MWJmMjQjRFlZwbv8-8\" width=350 height=350>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore Progress Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_app.gui.progress import restore_progress_vars\n",
    "\n",
    "progress_filepath = './progress.yaml'\n",
    "\n",
    "progress_paths = restore_progress_vars(progress_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the ARHMM\n",
    "\n",
    "Fitting the ARHMM typically requires adjusting the `kappa` hyperparameter to achieve a target syllable duration (higher values of `kappa` lead to longer syllable durations). The target duration can be determined using change-points analysis or set heuristically to 0.3-0.4 seconds based on prior literature. In the code below, set `kappa` to `'scan'` to run a family of models with different `kappa` values and use the \"Get Best Model Fit\" cell to pick an optimal value. We recommend fitting for 100-200 iterations to pick `kappa`. For final model fitting, set `kappa` to the chosen value and fit for ~1000 iterations. \n",
    "\n",
    "__Note: if loading a model checkpoint, ensure the modeling parameters (especially the selected groups) are identical to that of the checkpoint. Otherwise the model will fail.__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from moseq2_model.gui import learn_model_command\n",
    "from moseq2_app.gui.progress import update_progress\n",
    "\n",
    "modeling_session_path = 'saline-amphetamine/'\n",
    "model_name = 'model.p'\n",
    "\n",
    "session_path = join(progress_paths['base_dir'], modeling_session_path)\n",
    "model_path = join(session_path, model_name) # path to save trained model\n",
    "\n",
    "select_groups = False # select specific groups to model; if False, will model all data as is in moseq2-index.yaml\n",
    "\n",
    "# model saving freqency (in interations); will create a checkpoints/ directory containing checkpointed models\n",
    "checkpoint_freq = -1\n",
    "use_checkpoint = False # resume training from latest saved checkpoint\n",
    "\n",
    "# Advanced modeling parameters\n",
    "hold_out = False # boolean to hold out data subset during the training process\n",
    "nfolds = 2 # (if hold_out==True): number of folds to hold out during training; 1 fold per session\n",
    "\n",
    "npcs = 10  # number of PCs being used; base case should be npcs should have explained variance >= ~90% \n",
    "max_states = 100 # number of maximum states the ARHMM can end up with\n",
    "\n",
    "# use robust-ARHMM with t-distribution -> able to tolerate more noise\n",
    "robust = True \n",
    "\n",
    "# separate group transition graphs; set to True if you want to compare multiple groups\n",
    "separate_trans = True \n",
    "\n",
    "num_iter = 100 # number of iterations to train model\n",
    "\n",
    "# syllable length probability distribution prior; (None, int or 'scan'); if None, kappa=nframes\n",
    "kappa = None \n",
    "\n",
    "# if kappa == 'scan', optionally set bounds to scan kappa values between, in either a linear or log-scale.\n",
    "scan_scale = 'log' # or linear\n",
    "min_kappa = None\n",
    "max_kappa = None\n",
    "out_script = 'train_out.sh' # script file to save kappa-scanning learn_model() commands \n",
    "\n",
    "# total number of models to spool\n",
    "n_models = 15\n",
    "\n",
    "# Select platform to run models on\n",
    "cluster_type = 'local' # currently supported cluster_types = 'local' or 'slurm'\n",
    "run_cmd = False # if True, runs the commands via os.system(...), script must be run manually otherwise\n",
    "\n",
    "## SLURM PARAMETERS\n",
    "## only edit these parameters if cluster_type == 'slurm'\n",
    "memory = '16GB'\n",
    "wall_time='3:00:00'\n",
    "partition='short'\n",
    "\n",
    "progress_paths = update_progress(progress_filepath, 'model_path', model_path)\n",
    "progress_paths = update_progress(progress_filepath, 'model_session_path', session_path)\n",
    "\n",
    "learn_model_command(progress_paths, hold_out=hold_out, nfolds=nfolds, num_iter=num_iter, max_states=max_states,\n",
    "                    npcs=npcs, kappa=kappa, separate_trans=separate_trans, robust=robust,\n",
    "                    checkpoint_freq=checkpoint_freq, use_checkpoint=use_checkpoint, select_groups=select_groups,\n",
    "                    cluster_type=cluster_type, min_kappa=min_kappa, scan_scale=scan_scale,\n",
    "                    max_kappa=max_kappa, n_models=n_models, run_cmd=run_cmd, output_dir=modeling_session_path,\n",
    "                    out_script=out_script, memory=memory, wall_time=wall_time, partition=partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once training is complete, your model will be saved in your base directory (shown below). \n",
    "```\n",
    ".\n",
    "├── _pca/ \n",
    "├── aggregate_results/ \n",
    "├── config.yaml\n",
    "├── modeling_session/ ***\n",
    "├   └── model.p ***\n",
    "├── moseq2-index.yaml/\n",
    "├── session_1/\n",
    "└── session_2/\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore Notebook Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moseq2_app.gui.progress import restore_progress_vars\n",
    "\n",
    "progress_filepath = './progress.yaml'\n",
    "\n",
    "progress_paths = restore_progress_vars(progress_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Best Model Fit\n",
    "\n",
    "Use this feature to determine whether the trained model has captured median syllable durations that match the principal components changepoints.\n",
    "\n",
    "This feature can also return the best model from a list of models found in the `progress_paths['model_session_path']`.\n",
    "\n",
    "Below are examples of some comparative distributions that you can expect when using this tool:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img height=400 width=400 src=\"https://drive.google.com/uc?export=view&id=1B6R4AGsQHaddwJj-48Pbd_5ZHOZvpttp\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img height=400 width=400 src=\"https://drive.google.com/uc?export=view&id=1poLAAhNlAdM8T_1Ps6OMs6vNz03NGbgr\">\n",
    "        </td>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from moseq2_viz.gui import get_best_fit_model\n",
    "from moseq2_app.gui.progress import update_progress\n",
    "\n",
    "output_file = join(progress_paths['plot_path'], 'model_vs_pc_changepoints')\n",
    "\n",
    "best_model_fit = get_best_fit_model(progress_paths, plot_all=True)\n",
    "progress_paths = update_progress(progress_filepath, 'model_path', best_model_fit['best model - duration'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Notebook End\n",
    "\n",
    "## Go to the [Interactive-Model-Results.ipynb](./Interactive-Model-Results-Exploration.ipynb) Jupyter Notebook to analyze model results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "303.993px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
